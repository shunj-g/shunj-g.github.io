<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>自组织映射(self-organizing-map)SOM神经网络</title>
      <link href="/2018/10/05/2018-10-05/"/>
      <url>/2018/10/05/2018-10-05/</url>
      <content type="html"><![CDATA[<h1 id="自组织映射-self-organizing-map"><a href="#自组织映射-self-organizing-map" class="headerlink" title="自组织映射(self-organizing-map)"></a>自组织映射(self-organizing-map)</h1><p>自组织映射(self-organizing-map)SOM神经网络<br>自组织映射的主要目的是将任意维数的输入信号模式转变为一维或者二维的离散映射。并且以拓扑有序的方式自适应实现这个变换。<br>网格中的每个神经元和输入层的源节点全连接，这个网络表示具有神经元按行和列排列的单一计算层的前馈结构[1,2]。<br>在自组织映射形成过程种有三种主要的过程：</p><ol><li>竞争。多每个输入模式，网络中计算他们各自的判别的函数值，这个判别函数为神经元之间的竞争提供了竞争的基础。判别函数的最大值的神经元为竞争的胜利者。</li><li>合作。获胜的神经元决定兴奋神经元的拓扑领域的空间位置，从而提供这样的相邻神经元合作基础</li><li>突触调节。这个机制使兴奋神经元通过对他们图抽权值的适当调节以增加他们关于该输入模式的判别函数值。所做的调节使获胜的神经元对以后相似输入模式的响应增强了。</li></ol><h1 id="关于自组织映射的算法"><a href="#关于自组织映射的算法" class="headerlink" title="关于自组织映射的算法"></a>关于自组织映射的算法</h1><p>相应的核定义如下：</p><script type="math/tex; mode=display">k(x,w_i,\sigma _{i})=\frac{1}{\Gamma \left( \frac{m}{2} \right)}\Gamma \left( \frac{m}{2},\frac{\left\| x-w_i \right\|^2}{2\sigma _i^2} \right) i=1,2,\cdots ,l</script><p>注意以$r = ||x - w_i||$为中心的核对于所有的$i$是放射状对称的，不忘全的gamma分布保证了当输入分布是高斯时核的微分最大的。<br>根据上式，我们可以为构造自组织拓扑公式的算法做了准备，在映射中利用核函数来描述每个神经元。<br>通过以下式子的微分熵定义的目标函数对于核参数(<strong>权值向量$w_i$和核宽$\sigma_i$</strong>的梯度公式来开始)</p><script type="math/tex; mode=display">H(Y_i)=-\int_{-\infty }^{\infty }p_{Y_i}(y_i)\log p_{y_i}(y_i)dy_i</script><p>目标函数$H(Y_i)$是定义在第$i$个神经输出之上。</p><script type="math/tex; mode=display">y_i = k(x,w_i,\sigma_i)，i = 1,2,...,l</script><p>在另一个方面，通过定义在到核的中心的半径距离$r$之上，</p><script type="math/tex; mode=display">P_R(r)=\begin{cases} \frac{1}{2^{(m/2)-1}}\Gamma (m/2) {(\frac{r}{\sigma })}^{m-1}\exp (-\frac{r^2}{2\sigma^2}) & r \ge 0\newline 0 & r < 0 \end{cases}</script><p>将随机变量$R$变换到$Y_i$，且得到：</p><script type="math/tex; mode=display">p_{Y_i}(y_i)=\frac{p_R(r)}{\left|\frac{dy_i}{dr} \right|}</script><p>定义目标函数;</p><script type="math/tex; mode=display">H(Y_i)=-\int_{0}^{\infty }p_{R(r)}\log p_{R(r)}dr+\int_{0}^{\infty }{p_{R(r)}\log \left| \frac{\partial y_i(r)}{\partial r} \right|}dr</script><h2 id="考虑权值向量-w-i-的梯度"><a href="#考虑权值向量-w-i-的梯度" class="headerlink" title="考虑权值向量$w_i$的梯度"></a>考虑权值向量$w_i$的梯度</h2><p>对权值向量$w_i$的梯度，上式第一部分独立于$w_i$，第二项是偏导数$log|{\partial y_i(r)}/{\partial r}|$的期望。因此可以将$H(Y_i)$对于$w_i$的导数表达为：</p><script type="math/tex; mode=display">\frac{\partial H(Y_i)}{\partial w_i} = \frac{\partial E\left[log \left|  \frac{\partial y_i(r)}{\partial r}\right|\right]}{\partial w_i}</script><p>将右端项的期望用确定的量来替代：</p><script type="math/tex; mode=display">E \left[log \left| \frac{\partial y_i(r)}{\partial r}   \right|   \right] = log \left|  \frac{\partial \bar{y_i}(r)}{\partial r}\right|</script><p>选择一个启发式的方式：使用平方欧几里得项$||x-w_i||^2$,则可以通过以下方式来逼近：</p><script type="math/tex; mode=display">\frac{\partial H(\bar(y)_i}{\partial w_i}\approx \frac{x-w_i}{m\sigma_i^2}，\forall i</script><p>最大化目标函数，权值跟新与梯度上升相一致。于是可以得到：</p><script type="math/tex; mode=display">\Delta w_i = \eta_w \left( \frac{\partial H(\bar{y}_i)}{\partial w_i}\right)</script><p>$ \eta_w$是小的学习率参数。将输入向量x的固定维数m吸收到$\eta$。最后可以表示权值更新为：</p><script type="math/tex; mode=display">\Delta w_i \approx \eta_w \left( \frac{x-w_i}{\sigma_i^2}\right)</script><p>因此关于核SOM算法的第一个更新公式为：</p><script type="math/tex; mode=display">w_i^+ = w_i + \Delta w_i = w_i +\eta_w \left( \frac{x-w_i}{\sigma_i^2}\right)</script><p>其中$w_i$和$w_i^+$ 分别表示老的和更新后的神经元$i$的权值向量的值。</p><h2 id="对于核宽-sigma-i-的梯度向量"><a href="#对于核宽-sigma-i-的梯度向量" class="headerlink" title="对于核宽$\sigma_i$的梯度向量"></a>对于核宽$\sigma_i$的梯度向量</h2><p>关于目标函数$H(\bar{y}_i)$对于核宽$\sigma_i$的梯度向量。同对于权值向量的处理方式对梯度向量$\partial H(\bar{y}_i)/(\partial w_i)$相似的处理方式进行处理，得到：</p><script type="math/tex; mode=display">\frac{\partial H(\bar{y}_i)}{\partial \sigma_i} = \frac{1}{\sigma_i} \left(  \frac{||x-w_i||^2}{m\sigma_i^2}-1 \right)</script><p>调整核宽：</p><script type="math/tex; mode=display">\Delta \sigma_i = \eta_{\sigma}\frac{\partial H(\bar{y}_i)}{\partial sigma_i}= \frac{\eta_{\sigma}}{\sigma_i} \left( \frac{||x-w_i||^2}{m\sigma_i^2}-1 \right)</script><p>其中$\eta_{\sigma}$为第二个学习率参数。对于核SOM算法的第二个更新公式，我们有：</p><script type="math/tex; mode=display">\sigma_i^+ = \sigma_i+\Delta \sigma_i = \sigma_i + \frac{\eta_{\sigma}}{\sigma_i} \left( \frac{||x-w_i||^2}{m\sigma_i^2}-1 \right)</script><h1 id="拓扑映射构造"><a href="#拓扑映射构造" class="headerlink" title="拓扑映射构造"></a>拓扑映射构造</h1><p>考虑到由$l$个神经元组成的网格$A$，这些神经元是由相应的核集(不完全的gamma分布的补)来刻画:</p><script type="math/tex; mode=display">k(x,w_i,\sigma_i)，i = 1,2,...,l</script><p>为了构造拓扑映射，我们引入基于活跃程度的在网格$A$的$l$个神经元之间的竞争，获胜的神经元被定义为：</p><script type="math/tex; mode=display">i(x) = arg \max_i y_i(x)  当 j \in A</script><p>引入邻域函数$h_{j,i(x)}$，以获胜的神经元$i(x)$为中心，采用距获胜神经元$i(x)$的网格距离的单调减函数，则：</p><script type="math/tex; mode=display">h_{j,i(x)} = exp \left( -\frac{||x_j-w_i||^2}{2\sigma^2} , j \in A\right)</script><p>这里的$\sigma$记领域函数$h_{j,i(x)}$d的范围；不要讲领域范围$\sigma$和核宽$\sigma_i$相混淆。</p><h1 id="SOM算法的总结"><a href="#SOM算法的总结" class="headerlink" title="SOM算法的总结"></a>SOM算法的总结</h1><p>自组织的映射的步骤如下：</p><blockquote><ol><li>初试化。对初始权值向量$w_i(0)$和核宽$\sigma_i(0)(i =1,2,…,l)$选择随机值，这里$l$是网格结构中神经元的总的个数。这里仅有的限制是对不同的神经元$w_i(0)$和$\sigma_i(0)$也不同。</li><li>取样。从输入的分布中按一定的概率取出一个样本$x$.</li><li>相似度匹配。在算法的时间步$n$，用下面的准则来确定获胜的神经元$i(x)$:<pre><code> $$i(x) = arg\max_i y_j(x)，j = 1,2,...,l$$</code></pre></li><li>自适应。调整权值向量和每个核的宽，使用相应的更新公式：<script type="math/tex; mode=display">w_j(n+1) =\begin{cases} w_j(n)+\frac{\eta_w h_{j,i(x)}}{\sigma_j^2}(x(n)-w_j(n)), & j \in A    \newline  w_j(n), & else \end{cases}</script><script type="math/tex; mode=display">\sigma_j(n+1) = \begin{cases} \sigma_j(n) + \frac{\eta_{\sigma} h_{j,i(x)}}{\sigma_j(n)} \left[ \frac{||x(n)-w_j(n)||^2}{m\sigma_j^2(n)}-1\right],&j\in A\newline \sigma_j(n),&else \end{cases}</script>这里$\eta_w$和$\eta_{\sigma}$为学习算法的两个学习率参数，$h_{j,i(x)}$是以获取神经元$i(x)$为中心的领域函数</li></ol></blockquote>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN卷积神经网络之LeNet5原理</title>
      <link href="/2018/10/04/2018-10-04/"/>
      <url>/2018/10/04/2018-10-04/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>一个典型的卷积神经网络，如LeNet，开始阶段都是卷积层以及池化层的相互交替使用，之后采用全连接层将卷积层和池化之后的结果的特征全部提取进行概率计算处理。在具体的误差反馈和权重的更新的处理上，不论是全连接层发的更新还是卷积层的更新。使用的都是经典的反馈神经网络算法，这种方法将原本较为复杂的，要考虑长期的链式法则转化为只需要考虑前后节点输入和输出误差的权重的影响，使得神经网络当神经网络深度加深时能利用计算机计算，以及卷积核在计算的过程中产生了很多的数据计算。</p><h1 id="BP神经网络的算法的原理"><a href="#BP神经网络的算法的原理" class="headerlink" title="BP神经网络的算法的原理"></a>BP神经网络的算法的原理</h1><p>为了使得整个计算的过程统一，设一个敏感度的参数$\sigma_k$，即当前的输出层的误差对该层的输入的偏导数。<br>一般而言，经典的反馈网络有伴包括三个部分：数据的前向计算，误差的反向传播和权重的更新。</p><h2 id="1-前向传播算法"><a href="#1-前向传播算法" class="headerlink" title="1. 前向传播算法"></a>1. 前向传播算法</h2><p>对于前向传播算法的值传递，隐藏层输出的值的定义如下：</p><script type="math/tex; mode=display">a_h^{H1} = W_h^{H1} \cdot X_i</script><script type="math/tex; mode=display">b_h^{H1} = f(a_h^{H1} )</script><p>对于输出层，定义如下：</p><script type="math/tex; mode=display">a_k = \sum W_{hk} \cdot b_h^{H1}</script><p>其中，$X_i$是当前节点的输入值，$W_h^{H1}$是连接到此节点的权重，$a_h^{H1}$是输出值。$f$是当前阶段的激活函数，$b_h^{H1}$为当前节点的输入值经过计算后被激活的值。$W_{hk}$为输入的权重，$b_h^{H1}$为输入到输出节点的输入值。这里对所有输入的值进行权重计算后求得和值。</p><h2 id="2-反向传播算法"><a href="#2-反向传播算法" class="headerlink" title="2. 反向传播算法"></a>2. 反向传播算法</h2><p>与前向传播相似，首先定义两个值$\sigma_k$与$\sigma_h^{H1}$:</p><script type="math/tex; mode=display">\sigma_k = \frac{\partial Loss}{\partial a_k} = (Y-T)</script><script type="math/tex; mode=display">\sigma_h^{H1} = \frac{\partial Loss}{\partial a_h^{H1}}</script><p>其中，$\sigma_k$ 为输出层的误差项，是计算值($Y$)与模型计算值($T$)之间的差值。$\sigma_h^{H1}$为输出层的误差<br>反馈神经网络计算公式定义如下：</p><script type="math/tex; mode=display">\begin{align} \sigma_h^{H1} &= \frac{\partial Loss}{\partial a_h^{H1}}\newline &= \frac{\partial Loss}{\partial b_h^{H1}} \cdot \frac{\partial b_h^{H1}}{\partial a_h^{H1}} \newline & =\frac{\partial Loss}{\partial b_h^{H1}} \cdot f'(a_h^{H1}) \newline &= \frac{\partial Loss}{\partial a_k} \cdot \frac{\partial a_k}{\partial b_h^{H1}}  \cdot f'(a_h^{H1}) \newline &= \sigma_k \cdot \sum W_{hk} \cdot f'(a_h^{H1}) \newline &= \sum W_{hk} \cdot \sigma_k \cdot f'(a_h^{H1}) \end{align}</script><p>即当前层输出值对误差的梯度可以通过下一层的误差与权重和输入值的梯度乘积获得。<br>或者换一种表述形式将上面的公式表示为：</p><script type="math/tex; mode=display">\sigma^l= \sum W_{ij}^l \times \sigma_j^{l+1} \times f'(a_i^l)</script><h2 id="3-权重的更新"><a href="#3-权重的更新" class="headerlink" title="3. 权重的更新"></a>3. 权重的更新</h2><p>反馈的神经网络计算的目的是对权重的更新。与梯度下降法类似，式子如下：</p><script type="math/tex; mode=display">W_{ji} = W_{ji} + \alpha (f(\theta)-y_i)x_i</script><script type="math/tex; mode=display">b_{ji} = b_{ji} + \alpha \sigma_j^l</script><p>其中，$ji$表示为反向传播时对应的节点系数，通过对$\sigma_j^l$的计算来更新对应的权重。</p><h1 id="卷积神经网络正向和反向传播公式推导"><a href="#卷积神经网络正向和反向传播公式推导" class="headerlink" title="卷积神经网络正向和反向传播公式推导"></a>卷积神经网络正向和反向传播公式推导</h1><p>对于每层$l$的下面都会有一个采样层$l+1$,对于反馈神经网络而言，要求得$l$层的每个神经元对应的权值更新，就需要先求$l$层的每一个神经节点的灵敏度$\sigma_k$<br>即：正向的计算：</p><ul><li>输入层-卷积层</li><li>卷积层-池化层</li><li>池化层-全连接层</li><li>全连接层-输出层</li></ul><p>在正向计算的时候，当权重更新的时候，需要对其进行反向更新</p><ul><li>输出层-全连接层</li><li>全连接层-池化层</li><li>池化层-卷积层</li><li>卷积层-输入层</li></ul><p>相对于反馈网络，卷积神经网络在整个模型的构成上是分解为若干个小的步骤进行，因此对其进行求导更新计算的是逐层更新，<br>首先设定损失函数，因为采用个的one-hot方法，因此误差计算的误差函数为交叉熵函数：</p><script type="math/tex; mode=display">Loss = -ylog(f'(x)) + \frac{\lambda}{2} ||w||^2</script><h2 id="1-输出层到全连接层的方向求导"><a href="#1-输出层到全连接层的方向求导" class="headerlink" title="1.输出层到全连接层的方向求导"></a>1.输出层到全连接层的方向求导</h2><p>对于输出层来说，损失函数是交叉熵函数，由于one-hot方法是大部分是0，只有一个是1，对其交叉熵进行求解可得到：</p><script type="math/tex; mode=display">\begin{align} Loss(f(x),y) &= -\sum ylog(f(x)) \newline &= -(0 \times log(f(x_1)) +0 \times log(f(x_2))... 0 \times log(f(x_{n-1})) + 1\times log(f(x_n))) \newline &= -log(f(x_n))\end{align}</script><p>使用此种规则可以得到的损失函数为：</p><script type="math/tex; mode=display">Loss = -(y-log(f(x))) + \frac{\lambda}{2} ||w||^2</script><p>其中 $y$是样本值等于1的那个值，$log(f(x))$为模型计算出的交叉熵的那个值，其差值$y-log(f(x))$是误差的额度，因为y在one-hot中始终未1，化简如下：</p><script type="math/tex; mode=display">Loss = -(1-log(f(x))) + \frac{\lambda}{2} ||w||^2</script><p>我们的最终的输出层是$softmax$函数，那么对于结果会采用交叉熵的形式去计算损失函数，最后一层的误差敏感度就是卷积网络模型的输出值与真实值之间的差值。<br>那么根据损失函数对权值的偏导数，可以求得全连接的权重更新的计算公式：</p><script type="math/tex; mode=display">\frac{\partial Loss}{\partial w}  = -\frac{1}{m} \cdot (1-f(x)) \cdot f'(x) + \lambda w</script><p>其中，$f(x)$是激活函数，$w$是为$l-1$层到$l$层之间的权重。<br>则输出层的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial Loss}{\partial b}  = -\frac{1}{m} \cdot (1-f(x))</script><p><code>注：具体的推导如下：</code></p><blockquote><p>当$i = j$时，$softmax(f(x))$函数对于$f(x)$求导得到的导数为：</p><script type="math/tex; mode=display">\frac{\partial softmax(f(x))}{\partial f(x)} = f(x) \cdot (1-f(x))</script><p>对于$Loss(f(x))$对$f(x)$求导可以得到的导数如下：</p><script type="math/tex; mode=display">\frac{\partial Loss(f(x)}{\partial f(x)} = \frac{1}{f(x)} \cdot f'(x)</script><p>其中$f(x) = wx + b$<br>但是对于$\frac{1}{m}$是怎么来的有待下次推导</p></blockquote><h2 id="2-池化层反馈到卷积层的反向求导"><a href="#2-池化层反馈到卷积层的反向求导" class="headerlink" title="2. 池化层反馈到卷积层的反向求导"></a>2. 池化层反馈到卷积层的反向求导</h2><p>从正向来看，假设$l$层为卷积层，而$l+1$层为池化层<br>此时假设：池化层的敏感度为$\sigma_j^{l+1}$，卷积层的敏感度为$\sigma_j^l$。<br>两者的关系可以近似的表达为：</p><script type="math/tex; mode=display">\sigma_j^l = pool \left(\sigma_j^{l+1} \right) * h \left(a_j^l \right)</script><p>$*$对应的是点对点乘，即时元素对应位置的乘积。<br>如果按照mean-pool方法进行反馈运算，则需要将$l+1$池化层扩展到$l$卷积层的大小 ：矩阵由$2 \times 2$变为$4 \times 4$<br>的大矩阵，然后进行平摊处理得到最右边的结果<br>$\begin{bmatrix}1&amp; 2  \newline 3 &amp;4\end{bmatrix}$==&gt;$\begin{bmatrix}1&amp;1 &amp;2 &amp; 2  \newline 1&amp;1 &amp;2 &amp; 2   \newline 3&amp;3 &amp;4 &amp; 4  \newline3&amp;3 &amp;4 &amp; 4  \end{bmatrix}$==&gt;$\begin{bmatrix}0.25&amp;0.25 &amp;0.75&amp; 0.75  \newline 0.25&amp;0.25 &amp;0.75&amp; 0.75   \newline 0.5&amp;05 &amp;1 &amp;1  \newline0.5&amp;05 &amp;1 &amp;1  \end{bmatrix}$</p><h2 id="3-卷积层反馈到池化层的反向求导"><a href="#3-卷积层反馈到池化层的反向求导" class="headerlink" title="3. 卷积层反馈到池化层的反向求导"></a>3. 卷积层反馈到池化层的反向求导</h2><p>当$l$层为池化层的时候，而$l+1$层为卷积层时<br>假设第$l$层有$n$个通道，即有n张特征图[weight,height,n]，而在$l+1$卷积层中有m个特征值。如果$l$层池化层中每个通道都有其对应的敏感度误差，则计算依据为$l+1$层的卷积层中所有卷积核元素敏感度之和</p><script type="math/tex; mode=display">\sigma_j^l = \sum_j^m \left( \sigma_j^{l+1}\otimes K_{ij}\right)</script><p>其中$\otimes$是矩阵的卷积操作，与向前传播不同的是：求$l$层池化层对$l+1$层的敏感度是全卷积操作。在tensorflow中将padding设置为full。注意的是这里的卷积操作是将卷积核先旋转180度再与padding后的矩阵进行卷积计算。</p><h2 id="4-通过敏感度更新卷积神经网络中的权重"><a href="#4-通过敏感度更新卷积神经网络中的权重" class="headerlink" title="4.通过敏感度更新卷积神经网络中的权重"></a>4.通过敏感度更新卷积神经网络中的权重</h2><p>前面已经计算了在卷积神经网络中所有层中敏感度 ，对于卷积神经网络而言，其中特殊的是卷积层和池化层的权重更新比较难计算，而这些层的计算可以通过权重所连接的前后的敏感度来计算，则：</p><script type="math/tex; mode=display">\frac{\partial Loss}{\partial w_{ij}} = x_i \cdot \sigma_j^{i+1}</script><script type="math/tex; mode=display">\frac{\partial Loss}{\partial b_{ij}} = \sum \left( \sigma_j^{i+1}\right)</script><p>其中$\cdot$表示矩阵的点乘<br>未完待更新。。。。。。</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> LeNet5 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CNN卷积神经网络之TensorFlow的接口</title>
      <link href="/2018/10/03/2018-10-03/"/>
      <url>/2018/10/03/2018-10-03/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>卷积神经网络时从信号处理衍生而来的一种对数字信号的处理的方式，发展到图像信号处理上演变为一种专门用来处理具有矩阵特征的网格结构处理方式。CNN在很多的应用上的有独特的优势，例如音频处理和图像处理。在数字图像处理中有一种基本的处理方法名为：线性滤波。其中的滤波的工具就是一个小型的矩阵，即卷积核。卷积核的大小远远小于图像矩阵本身。</p><h1 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h1><p>卷积实际上两个矩阵进行一种数学运算。</p><script type="math/tex; mode=display">s(t) = \int x(a) \omega(t-a)da</script><p>该式子是卷积运算，另外的表示方式：</p><script type="math/tex; mode=display">s(t) = (x \cdot \omega)(t)</script><p>在公式中，第一个参数的$x$被称为”输入数据“，第二个参数的$\omega$被称为”核函数“，$s(t)$是输出，即特征映射。<br>数字图像的卷积主要有两种思维：1. 稀疏矩阵，2. 参数共享<br>首先对于稀疏矩阵而言，卷积网络具有稀疏性，即卷积核的大小远远输入矩阵的大小。这样就能够计算出更小的参数特征，极大的减少后续的计算量<br>其次，参数共享是指在特征提取过程中，一个模型在多个参数之中使用相同的参数，在传统的神经网络中，每个权重只对应其连接的输入输出起作用，其连接的输入输出的袁术结束后就不会再用到。卷积神经网络中核的每一个元素都被作用输入的每个位置上，在过程中只需要学习一个参数集合就能够用将这些参数用到所有的图片元素中。</p><h1 id="TensorFlow中卷积函数"><a href="#TensorFlow中卷积函数" class="headerlink" title="TensorFlow中卷积函数"></a>TensorFlow中卷积函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.nn.conv2d(input,filter,strides,padding,use_cudnn_on_gpu = <span class="keyword">None</span>,name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>这里的核心参数有5个：<br>&gt;<br> <strong><em>input</em></strong>: 需要做卷积的图像,Tensor结构，具有[batch,in_height,in_width,in_channels]这样的shape,具体的含义是[训练一个batch的图片数量，图片高度，图片宽度，图片通道数]，该tensor是四维的，数据的类型是32和64的float类型。<br><strong><em>filter</em></strong>:相当于CNN的卷积核，他的要求是一个Tensor,具有[filter_height,filter_width,in_channels,out_channels]这样的shape，具体的含义是：[卷积核高度，卷积核宽度，图像通道数，卷积核个数]，要求的类型与参数的input相同，<code>注：第三维in_channels，就是参数input的第四维</code><br><strong><em>strides</em></strong>:卷积时，图像的每一维的步长，这个是一个一维的向量，第一位和第四维一般默认为为1，第三维和第四维分别是平行和垂直的滑行的步长<br><strong><em>padding</em></strong>:string类型的量，只能是“SAME”和“VALID”这个决定了不同的卷积方式。当生成的卷积结果和原输入的矩阵的大小一致时，使用“VALID”,当使用“SAME”时，表示图像的边缘由一圈0补齐，使得卷积后的图像大小和输入的大小一致。<br><strong><em>use_cudnn_on_gpu</em></strong>：bool类型，是否使用cudnn加速，默认为true</p><h1 id="池化运算-对不同的位置特征进行聚合统计"><a href="#池化运算-对不同的位置特征进行聚合统计" class="headerlink" title="池化运算-对不同的位置特征进行聚合统计"></a>池化运算-对不同的位置特征进行聚合统计</h1><p>通过卷积得到特征后(features)之后，下一步就应该做分类了；然而，数据量太大（一张28<em>28的图像，得到了400个4</em>4输入上的特征，每一个特征和图像卷积都会得到一个(28-4+1)<em>(28-4+1) =625维的特征向量 ，由于有400个特征向量，每个样例（sample）都会得到一个625</em>400 = 250000维的特征向量，这个虽然不够大，，但是，当图像的维数增大时，则计算复杂度增大，例如当维数为96*96时，则计算的数据量为300多万。这个对于一个分类器的压力是很大的，这样就会容易出现过拟合(over-fitting)的现象）<br>这个问题的产生主要是卷积后的图像具有一种”静态性“的属性，也就是说，一个区域有用的特征在另外的特征极有可能在另外的区域也能够适用。因此就有必要对不同的位置的特征进行聚合统计，即平均池化，最大池化<br>池化相同的或者重复的隐藏单元的产生的特征，那么池化单元具有平移不变性（translation invariant）,这个意味图像经管平移一个区域之后，仍然能产生很多相同的（池化）特征。在(物体检测和语音识别中）希望得到更多的平移不变性的特征<br>Tensorflow中池化运算的函数如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.nn.max_pool(value,ksize,strides,pading,name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p><p>参数有四个，和卷积很类似</p><blockquote><p><strong><em>value</em></strong>： 需要池化的输入，一般池化层在卷积层之后，所以输入的是feature_map，依然是[batch,height,width,channels]这样的shape<br><strong><em>ksize</em></strong>: 池化窗口的大小，取一个思维的向量，一般是[1,height,width,1]，一般不再batch和channels上做池化，所以这两个维度设为1.<br><strong><em>strides</em></strong>: 和卷积很类似，窗口的每一个维度上滑动的步长，一般是[1,stride,stride,1]<br><strong><em>padding</em></strong>:和卷积和类似，可以取“VALID”或者是“SAME”,返回的是一个Tensor，类型不变，shape仍然是[batch,height,width,channels]</p></blockquote>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>三类聚类算法</title>
      <link href="/2018/09/09/isodata/"/>
      <url>/2018/09/09/isodata/</url>
      <content type="html"><![CDATA[<h1 id="k均值聚类算法（经典的）"><a href="#k均值聚类算法（经典的）" class="headerlink" title="k均值聚类算法（经典的）"></a>k均值聚类算法（经典的）</h1><p>算法步骤如下：</p><blockquote><p>step1: 从数据集中随机选取K个样本作为初始聚类中心$C = {c_1,c_2,…,c_k}$;<br>step2: 针对数据集中的每个样本$x_1$,计算它到$K$个聚类中心的距离，并且将其分到距离最小的聚类中心所对应的类中；<br>step3:针对每个类别$c_i$,从新计算它的聚类中心$c_i = \frac{1} {|c_i|} \sum_{x \in  c_i} x$ 属于这一类的所有样本的质心<br>step4:重复第二步和第三步直到聚类中心的位置不再变化</p></blockquote><h1 id="K均值-算法"><a href="#K均值-算法" class="headerlink" title="K均值++算法"></a>K均值++算法</h1><p>该算法是由$D.Arthur$等人提出来的K-means++针对经典的K均值算法的第一步做了改进，在2016年Olivier Bachem和Mario Lucic等人将K-means++做了改进，该论文在<a href="https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means" target="_blank" rel="noopener">NIPS2016</a>上</p><blockquote><p>step1: 从数据集中随机选取一个样本作为初始的聚类中心$c_1$;<br>step2:首先计算每个样本与当前的已有的聚类中心之间的最短距离(即与最近的一个聚类中心的距离)，用$D(x)$表示；接着计算每个样本被选为下一个聚类中心的概率$\frac{D(x)^2}{\sum_{x \in X} D(x)^2}$,最后，按照轮盘法选择下一个聚类中心；<br>step3：重复第二步的直到选出共$K$个聚类中心；<br>之后的过程与经典的$K-means$算法中的第2步和第4步相同</p></blockquote><h1 id="ISODATA算法"><a href="#ISODATA算法" class="headerlink" title="ISODATA算法"></a>ISODATA算法</h1><p>该算法较为复杂， K-means和K-means++的聚类中心数K是固定不变的，而ISODATA算法在运行过程中能够根据各个类别的实际情况进行两种操作来调整聚类中心数$K$,<br>ISODATA算法基本上有两个操作：</p><blockquote><p>（1）分裂操作，对应的增加中心数；<br>（2） 合并操作，对应发减少聚类中心数。</p></blockquote><p>关于ISODATA算法的输入参数的含义：</p><blockquote><p>（1）预期的聚类中心数目$K_0$: 用户指定一个运行的中心数目，最终的数目使用其决定的，输出的数目范围是$[K_0/2,2K_0]$.<br>（2）每个类所有的要求的最少的样本数目$N_{min}$:用于判断某个类别所包含的样本分散程度较大时是否可以进行分裂操作的临界值。<br>（3）最大方差$\sigma$：用于衡量某个类别中样本的分散程度，是分裂操作的临界值（与第二个条件需要同时满足）<br>（4）两个类别对应的聚类中心之间所允许的最小距离$d_{min}$：如果两个类别考的特别近，则需要对这两个类别进行合并操作，该值是决定是否分裂的临界值</p></blockquote><p>ISODATA算法的流程：</p><blockquote><p>step1: 从数据集中随机选取出$K_0$个样本作为初始聚类中心$C = {c_1,c_2,…,c_{k_o}}$;<br>step2: 针对数据集中每个样本$x_i$,计算它到$K_0$个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；<br>step3: 判断上述每个类中的元素数目是否小于$N_{min}$。如果小于N_{min}则需要丢弃该类，令$K = K-1$,并将该类中的样本重新分配给剩下类中距离最小的类；<br>step4:针对每个类别$c_i$,重新计算它的聚类中心$c_i = \frac{1}{|c_i|}\sum_{x \in c_i}x$(即属于该类的所有的质心)；<br>step5: 如果当前${K \leq \frac{k_0}{2} }$,说明当前类别数太少，<br>step6: 如果当前$K \geq 2K_0$，说明当前的类别数太多，前往合并操作；<br>step7:如果达到最大的迭代次数则算法终止，否则回到第二步区继续执行</p></blockquote><p>step5的合并操作：</p><blockquote><p>step1:计算当前所有的类别聚类中心凉凉之间的距离，用矩阵$D$表示，其中$D(i,i) = 0$;<br>step2:对于$D(i,j) \le d_{min}(i \ne j)$的两个类别需要进行合并操作，变成一个新的类，该类中心位置为：</p><script type="math/tex; mode=display">m_{new} = \frac{1}{n_i + n_j}(n_im_i + n_jm_j)</script><p>上式中的$n_i$和$n_j$表示这两个类别中的样本个数，新的聚类中心可以看作是对两个类别进行加权求和。如果其中一个类所把汗的样本个数较多，缩合成的新类就会更加偏向它。</p></blockquote><p>step6的分裂操作：</p><blockquote><p>step1:计算每个类别下的所有样本在每个维度下的方差；<br>step2:针对每个类别所有方差挑选出最大的方差$\sigma_{max}$;<br>step3:如果某个类别的$\sigma_{max} \ge Sigma$并且该类别包含的样本数量$n_i \geq 2n_{min}$,则可以进行分裂操作，前往步骤4，如果不满足上述条件则退分裂操作。<br>step4: 将满足步骤3中的条件的类分为两个子类别并令$K = K +1$,</p><script type="math/tex; mode=display">m_i^{(+)} = m_i+\sigma_{max} ,m_i^{(-)} = m_i-\sigma_{max}</script><p>该算法能够在聚类过程中根据各个类所包含样本的实际情况动态调整聚类中心的数目。如果某个类中样本分散程度较大（通过方差进行衡量）并且样本数量较大，则对其进行分裂操作；如果某两个类别靠得比较近（通过聚类中心的距离衡量），则对它们进行合并操作。</p></blockquote>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K-Means </tag>
            
            <tag> ISODATA </tag>
            
            <tag> K-Means++ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>通过BPTT计算RNN的梯度</title>
      <link href="/2018/08/12/%E9%80%9A%E8%BF%87BPTT%E8%AE%A1%E7%AE%97RNN%E7%9A%84%E6%A2%AF%E5%BA%A6/"/>
      <url>/2018/08/12/%E9%80%9A%E8%BF%87BPTT%E8%AE%A1%E7%AE%97RNN%E7%9A%84%E6%A2%AF%E5%BA%A6/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我们知道循环神经网络(RNN)中一些重要的设计模式包括以下几种：</p><blockquote><ol><li>每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如图 1 所示。</li><li>每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络，如图 2 所示。</li><li>隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如图 3 所示。</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/RNN_1.jpg" alt="git_image"></p><font size="2">图 1: 计算循环网络(将x值的输入序列映射到输出值$o$的对应序列) 训练损失的计算图。损失$L$ 衡量每个$o$ 与相应的训练目标y 的距离。当使用$softmax$输出时，我们假设$o$是未归一化的对数概率。损失L 内部计算$\hat{y} = softmax(o)$，并将其与目标y 比较。RNN输入到隐藏的连接由权重矩阵U 参数化，隐藏到隐藏的循环连接由权重矩阵$W$ 参数化以及隐藏到输出的连接由权重矩阵$V$ 参数化。式1定义了该模型中的前向传播。(左) 使用循环连接绘制的RNN 和它的损失。(右) 同一网络被视为展开的计算图，其中每个节点现在与一个特定的时间实例相关联。</font><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/RNN_2.jpg" alt="git_image"></p><font size="2">图 2: 此类RNN 的唯一循环是从输出到隐藏层的反馈连接。在每个时间步$t$，输入为$x_t$，隐藏层激活为$h(t)$，输出为$o(t)$，目标为$y(t)$，损失为$L(t)$。(左) 回路原理图。(右) 展开的计算图。这样的RNN 没有图 1 表示的RNN 那样强大（只能表示更小的函数集合）。图 1 中的RNN 可以选择将其想要的关于过去的任何信息放入隐藏表示$h$中并且将$h$传播到未来。该图中的RNN 被训练为将特定输出值放入$o$中，并且$o$是允许传播到未来的唯一信息。此处没有从$h$前向传播的直接连接。之前的$h$仅通过产生的预测间接地连接到当前。$o$通常缺乏过去的重要信息，除非它非常高维且内容丰富。这使得该图中的RNN不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化，</font><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/RNN_3.jpg" alt="git_image"></p><font size="2">图 3: 关于时间展开的循环神经网络，在序列结束时具有单个输出。这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标（如此处所示），或者通过更下游模块的反向传播来获得输出$o(t)$上的梯度。</font><p>现在我们研究图 1 中RNN的前向传播公式。这个图没有指定隐藏单元的激活函数。我们假设使用双曲正切激活函数。此外，图中没有明确指定何种形式的输出和损失函数。我们假定输出是离散的，如用于预测词或字符的RNN。表示离散变量的常规方式是把输出$o$作为每个离散变量可能值的非标准化对数概率。然后，我们可以应用$softmax$函数后续处理后，获得标准化后概率的输出向量$ \hat{y}$。RNN 从特定的初始状态$h(0)$开始前向传播。从$t = 1 $到$t = \tau$的每个时间步，我们应用以下更新方程：</p><script type="math/tex; mode=display">\begin{aligned} &a(t) = b + Wh^{(t-1)} + Ux(t); ..............(1) \newline &h(t) = tanh(a(t)); ..............(2)\newline &o(t) = c + Vh(t);..............(3) \newline &\hat{y}(t) = softmax(o(t)); ..............(4) \end{aligned}</script><p>其中的参数的偏置向量$b$和$c$连同权重矩阵$U$、$V$ 和$W$，分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的连接。这个循环网络将一个输入序列映射到相同长度的输出序列。与$x$序列配对的$y$的总损失就是所有时间步的损失之和。</p><h1 id="softmax的导数求解"><a href="#softmax的导数求解" class="headerlink" title="softmax的导数求解"></a>softmax的导数求解</h1><p>我们知道softmax函数是用来进行数据归一化的，一般用作结果数据处理，将数据归一化在(0,1)的区间内。其表达式为：</p><script type="math/tex; mode=display">S_i =\frac{e^i}{\sum_je^j}</script><p>对于一个神经网络结构如下图(图片来自网络)，其参数计算如下：</p><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/softmax00.png" alt="git_image"></p><p>我们可以计算出各个输出函数：</p><script type="math/tex; mode=display">z_4 = w_{41}*o_1+w_{42}*o_2+w_{43}*o_3</script><script type="math/tex; mode=display">z_5 = w_{51}*o_1+w_{52}*o_2+w_{53}*o_3</script><script type="math/tex; mode=display">z_6 = w_{61}*o_1+w{62}*o_2+w_{63}*o_3</script><p>那么我们可以经过softmax函数得到:</p><script type="math/tex; mode=display">a_4 = \frac{e^{z_4}}{Z^{z_4}+Z^{z_5}+Z^{z_6}}</script><script type="math/tex; mode=display">a_5 = \frac{e^{z_5}}{Z^{z_4}+Z^{z_5}+Z^{z_6}}</script><script type="math/tex; mode=display">a_6= \frac{e^{z_6}}{Z^{z_4}+Z^{z_5}+Z^{z_6}}</script><p>交叉熵函数形式为：</p><script type="math/tex; mode=display">Loss = -\sum_iy_ilna</script><p>进行求导计算：<br>如果$i = j$，则</p><script type="math/tex; mode=display">\begin{aligned}   \frac{\partial a_j}{\partial z_i} &= \frac{\partial}{\partial z_i}(\frac{e^{z_j}}{\sum_k e^{z_k}}) \newline &= \frac{(e^{z_j})^{\prime} \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_j}}{(\sum_k e^{z_k})^2} \newline &=\frac{ e^{z_j} }{\sum_k e^{z_k} } -\frac{ e^{z_j} }{\sum_k e^{z_k} } \cdot  \frac{ e^{z_j} }{\sum_k e^{z_k} } \newline &= a_j(1-a_j)\end{aligned}</script><p>如果$i \ne j$，则</p><script type="math/tex; mode=display">\begin{aligned}   \frac{\partial a_j}{\partial z_i} &= \frac{\partial}{\partial z_i}(\frac{e^{z_j}}{\sum_k e^{z_k}}) \newline &= \frac{ 0 \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_i}}{(\sum_k e^{z_k})^2} \newline &= -\frac{ e^{z_j} }{\sum_k e^{z_k} } \cdot  \frac{ e^{z_i} }{\sum_k e^{z_k} } \newline &= -a_ja_i\end{aligned}</script><h1 id="计算RNN的梯度"><a href="#计算RNN的梯度" class="headerlink" title="计算RNN的梯度"></a>计算RNN的梯度</h1><p>计算循环神经网络的梯度是容易的。由反向传播计算得到的梯度，并结合任何通用的基于梯度的技术就可以训练RNN。<br>为了获得BPTT算法行为的一些直观理解，我们举例说明如何通过BPTT计算上述RNN公式（式(1)）的梯度。计算图的节点包括参数$U,V,W,b$和$c$，以及以$t$ 为索引的节点序列$x(t),h(t),o(t)$和$L(t)$。对于每一个节点$N$，我们需要基于$N$后面的节点的梯度，递归地计算梯度$\nabla_NL$。我们从紧接着最终损失的节点开始递归：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial L^{(t)}} = 1</script><p>在这个导数中，我们假设输出$o(t)$作为$softmax$函数的参数，我们可以从$softmax$函数可以获得关于输出概率的向量$\hat{y}$。我们也假设损失是迄今为止给定了输入后的真实目标$y(t)$的负对数似然。对于所有$i,t$，关于时间步$t$输出的梯度$\nabla_{o(t)}L$如下：</p><script type="math/tex; mode=display">(\nabla_{o^{(t)}}L)_i = \frac{\partial L}{\partial o_{i}^{(t)}}  =\frac{\partial L }{\partial L^{(t)}} \frac{\partial L^{(t)} }{\partial o_{i}^{(t)}}= \hat{y}_{i}^{(t)} - 1_{i,y^{(t)}}</script><p><code>注：</code>这里将详细的计算推导如下：<br><code>这个只是个人的逻辑推导，精确推导有待时日再来</code></p><blockquote><p>$L(t)$ 为给定的$x^{(1)},..,x^{(t)} $后$y(t)$ 的负对数似然:</p><script type="math/tex; mode=display">\begin{aligned} L( x^{(1)},..,x^{(t)},y^{(1)},..,y^{(t)}) &= \sum_tL^{(t)}\newline  &=-\sum_tlog(p(y^{(t)}|x^{(1)},..,x^{(t)}) \end{aligned}</script><p>则$L^{(t)}$对$y^{(t)}$求导可以得到一下的结果：</p><script type="math/tex; mode=display">\frac{\partial L^{(t)}}{\partial y^{(t)}} \propto -\frac{1}{y^{(t)}}</script><p>根据之前的softmax函数的推导，我们可以得到  ：</p><script type="math/tex; mode=display">\frac{\partial L^{(t)} }{\partial o_{i}^{(t)}} = \frac{\partial L^{(t)}}{\partial y^{(t)}} \frac{ \partial y^{(t)}}{\partial o^{(t)}} = -\frac{1}{y^{(t)}} \cdot y^{(t)}(1-y^{(t)}) = y^{(t)}-1</script></blockquote><p>我们从序列的末尾开始，反向进行计算。在最后的时间步$\tau$, $h^{(\tau)}$只有$o^{(\tau)}$ 作为后续节点，因此这个梯度很简单：</p><script type="math/tex; mode=display">\nabla_{o^{(t)}}L = V^T \nabla_{o^{(t)}}L</script><p>然后，我们可以从时刻$t = \tau - 1$到$t = 1$反向迭代，通过时间反向传播梯度，注意$h(t)(t &lt; \tau)$ 同时具有$o(t)$和$h(t+1)$两个后续节点。因此，它的梯度由下式计算<br><code>这个式子看起来简单但是求解起来很容易出错，因为其中嵌套着激活函数函数，是复合函数的求道过程。</code></p><script type="math/tex; mode=display">\begin{aligned} \nabla_{h(t)}L &= (\frac{\partial h^{(t+1)}}{\partial h^{(t)}})^T(\nabla_{h(t+1)}L) + (\frac{\partial o^{(t)}}{\partial h^{(t)}})^T(\nabla_{o(t)}L) \newline   &=W^T( \nabla_{h(t+1)}L)diag(1-(h^{(t+1)})^2) + V^T(\nabla_{o(t)}L)\end{aligned}</script><p>其中$diag(1-(h^{(t+1)})^2)$ 表示包含元素  $1-(h_{i}^{(t+1)})^2$的对角矩阵。这是关于时刻$t+1$与隐藏单元i 关联的双曲正切的$Jacobian$。<br>一旦获得了计算图内部节点的梯度，我们就可以得到关于参数节点的梯度。因为参数在许多时间步共享，我们必须在表示这些变量的微积分操作时谨慎对待。我们希望实现的等式使用$bprop$ 方法计算计算图中单一边对梯度的贡献。然而微积分中的$∇_Wf$ 算子，计算$W$ 对于$f$ 的贡献时将计算图中的所有边都考虑进去了。为了消除这种歧义，我们定义只在$t $时刻使用的虚拟变量$W^{(t)}$ 作为W的副本。然后，我们可以使用$∇_{W^{(t)}}$ 表示权重在时间步$t $对梯度的贡献。<br>使用这个表示，关于剩下参数的梯度可以由下式给出：</p><script type="math/tex; mode=display">\begin{aligned} &\nabla_cL = \sum_{t}(\frac{\partial o^{(t)}}{\partial c})^T \nabla_{o^{(t)}}L=  \nabla_{o^{(t)}}L, \newline &\nabla_bL = \sum_{t}(\frac{\partial h^{(t)}}{\partial b^{(t)}})^T \nabla_{h^{(t)}}L=  \sum_{t}diag(1-(h^{(t)})^2)\nabla_{h(t)}L,\newline &\nabla_VL = \sum_{t}\sum_{i}(\frac{\partial L}{\partial o_{i}^{(t)}})^T \nabla_{V}o^{(t)}=  \sum_{t}(\nabla_{o(t)}L)h^{(t)^T}, \newline & \nabla_WL = \sum_{t}\sum_{i}(\frac{\partial L}{\partial h_{i}^{(t)}})^T \nabla_{W^{(t)}}h_{i}^{(t)} =  \sum_{t}diag(1-(h^{(t)})^2)(\nabla_{h(t)}L)h^{(t)^T},\newline & \nabla_UL = \sum_{t}\sum_{i}(\frac{\partial L}{\partial h_{i}^{(t)}})^T \nabla_{U^{(t)}}h_{i}^{(t)} =  \sum_{t}diag(1-(h^{(t)})^2)(\nabla_{h(t)}L )x^{(t)^T}, \end{aligned}</script><p>因为计算图中定义的损失的任何参数都不是训练数据$x^{(t)} $的父节点，所以我们不需要计算关于它的梯度。</p><h1 id="关于梯度消失和梯度爆炸"><a href="#关于梯度消失和梯度爆炸" class="headerlink" title="关于梯度消失和梯度爆炸"></a>关于梯度消失和梯度爆炸</h1><p>在累乘的过程中，如果取$sigmoid$函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。其实RNN的时间序列与深层神经网络很像，在较为深层的神经网络中使用$sigmoid$函数做激活函数也会导致反向传播时梯度消失，梯度消失就意味消失那一层的参数再也不更新，那么那一层隐层就变成了单纯的映射层，毫无意义了，所以在深层神经网络中，有时候多加神经元数量可能会比多家深度好。<br>RNN的特点本来就是能“追根溯源“利用历史数据，现在告诉我可利用的历史数据竟然是有限的，这就令人非常难受，解决“梯度消失“是非常必要的。解决“梯度消失“的方法主要有：<br>1、选取更好的激活函数<br>2、改变传播结构<br>关于第一点，一般选用$ReLU$函数作为激活函数，<br>ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。还有一点就是如果左侧横为0的导数有可能导致把神经元学死，不过设置合适的步长（学习旅）也可以有效避免这个问题的发生。<br>$sigmoid$函数还有一个缺点，$Sigmoid$函数输出不是零中心对称。$sigmoid$的输出均大于0，这就使得输出不是0均值，称为偏移现象，这将导致后一层的神经元将上一层输出的非0均值的信号作为输入。关于原点对称的输入和中心对称的输出，网络会收敛地更好。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BPTT </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ReLU激活函数</title>
      <link href="/2018/07/28/ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/2018/07/28/ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在深度神经网络中，通常使用一种叫修正线性单元(Rectified linear unit，ReLU）作为神经元的激活函数。ReLU起源于神经科学的研究：2001年，Dayan、Abott从生物学角度模拟出了脑神经元接受信号更精确的激活模型，如下图：<br><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/001.png" alt="git_image"><br>其中横轴是时间(ms)，纵轴是神经元的放电速率(Firing Rate)。同年，Attwell等神经科学家通过研究大脑的能量消耗过程，推测神经元的工作方式具有稀疏性和分布性；2003年Lennie等神经科学家估测大脑同时被激活的神经元只有1~4%，这进一步表明了神经元的工作稀疏性。</p><h1 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h1><p>首先，我们来看一下ReLU激活函数的形式，如下图：<br><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/002.png" alt="git_image"><br>从上图不难看出，ReLU函数其实是分段线性函数，把所有的负值都变为0，而正值不变，这种操作被成为单侧抑制。可别小看这个简单的操作，正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。其实还不一定是这样的形状。只要能起到单侧抑制的作用，无论是镜面翻转还是180度翻转，最终神经元的输出也只是相当于加上了一个常数项系数，并不影响模型的训练结果。之所以这样定，或许是为了契合生物学角度，便于我们理解吧。</p><p>那么问题来了：这种稀疏性有何作用？换句话说，我们为什么需要让神经元稀疏？当训练一个深度分类模型的时候，和目标相关的特征往往也就那么几个，因此通过ReLU实现稀疏后的模型能够更好地挖掘相关特征，拟合训练数据。</p><p>此外，相比于其它激活函数来说，<strong>ReLU有以下优势</strong>：对于线性函数而言，ReLU的表达能力更强，尤其体现在深度网络中；而对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此<strong>不存在梯度消失问题(Vanishing Gradient Problem)</strong>，使得模型的收敛速度维持在一个稳定状态。这里稍微描述一下什么是<strong>梯度消失问题</strong>：当梯度小于1时，预测值与真实值之间的误差每传播一层会衰减一次，如果在深层模型中使用sigmoid作为激活函数，这种现象尤为明显，将导致模型收敛停滞不前。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ReLU </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>隐马尔科夫模型</title>
      <link href="/2018/07/15/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/07/15/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>隐马尔科夫模型是关于时序的概率模型，描述一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔科夫随机生成的状态序列，称为状态序列(state sequence)；每个状态生成一个观测，而由此产生的观测的随机序列(observation sequence).。序列的每一个位置又可以看作一个时刻。<br>隐马尔科夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定。隐马尔科夫模型形式如下：<br>设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合</p><script type="math/tex; mode=display">Q={q_1,q_2,...,q_N}，V={v_1,v_2,...,v_M}</script><p>其中，$N$是可能的状态数，$M$是可能的观测数。<br>$I$是长度为$T$的状态序列，$O$是对应的观测序列。</p><script type="math/tex; mode=display">I=(i_1,i_2,...,i_T)，O=(o_1,o_2,...,o_T)</script><p>$A$是状态转移概率矩阵：</p><script type="math/tex; mode=display">A=[a_{ij}]_{N \times M}</script><p>其中，<script type="math/tex">a_{ij}=P(i_{t+1}=q_j|i_t=q_i)，i=1,2,...,N;j=1,2,...,N</script><br>是在时刻$t$处于状态$q_i$条件下载时刻$t+1$转移到状态$q_j$的概率。<br>B 是观测概率矩阵：<script type="math/tex">B=[b_j(k)]_{N \times M}</script><br>其中，<script type="math/tex">b_j(k)=P(o_t=v_k|i_i=q_i)，k=1,2,...,M;j=1,2,...,N</script><br>是在时刻$t$处于状态$q_j$的条件下生产观测$v_k$的概率。<br>$\pi$是初试状态概率向量：<script type="math/tex">\pi=(\pi_i)</script><br>其中，<script type="math/tex">\pi_i=P(i_1=q_i)，i=1,2,..,N</script><br>是时刻$t=1$处于状态$q_i$的概率。<br>隐马尔科夫模型由初试状态概率向量$\pi$，状态转移概率矩阵$A$和观测概率矩阵$B$决定。$\pi$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔科夫模型$\lambda$可以用三元符号表示，即：<script type="math/tex">\lambda=(A,B,\pi)</script><br>$A,B,\pi$称为隐马尔科夫模型的三要素。<br>状态转移概率矩阵$A$与初始状态概率向量$\pi$确定了隐藏的马尔科夫链，生成不可观测的状态序列。观测概率矩阵$B$确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。<br>从定义来看，隐马尔科夫模型作了两个基本的假设：<br>(1) 齐次马尔科夫性的假设，即假设隐藏的马尔科夫链在任意时刻$t$的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻$t$无关：</p><script type="math/tex; mode=display">P(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1})，t=1,2,...,T</script><p>(2) 观测独立性假设，即假设任意时刻的患侧只依赖于该时刻的马尔科夫链的状态，与其他观测状态无关。</p><script type="math/tex; mode=display">P(o_t|i_T,o_T,i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},...,i_1,o_1)=P(o_i|i_i)</script><h1 id="观测序列生成过程过程"><a href="#观测序列生成过程过程" class="headerlink" title="观测序列生成过程过程"></a>观测序列生成过程过程</h1><p>根据马尔科夫模型的定义，可以将一个长度为$T$的观测序列$O=(o_1,o_2,…,o_T)$的生成过程描述如下：</p><blockquote><p><strong>算法</strong> 观测序列形成<br>输入：隐马尔科夫模型$\lambda=(A,B,\pi)$，观测序列长度T<br>输出：观测序列$O=(o_1,o_2,…,o_T)$。<br>(1) 按照初始状态分布$\pi$产生状态$i_i$<br>(2) 令$t=1$<br>(3) 按照状态$i_t$的观测概率分布$b_{i_{t}}(k)$生成$o_t$<br>(4) 按照状态$i_t$的状态转移概率分布$\{a_{i_t,i_{t+1}}\}$产生状态$i_{t+1}$，$i_{t+1}=1,2,…,N$<br>(5) 令$t=t+1$;如果$t&lt;T$，转步(3);否则，终止</p></blockquote><h1 id="隐马尔科夫模型的三个基本问题"><a href="#隐马尔科夫模型的三个基本问题" class="headerlink" title="隐马尔科夫模型的三个基本问题"></a>隐马尔科夫模型的三个基本问题</h1><p>隐马尔科夫模型有3个基本问题：<br>(1) 概率计算问题，给定模型$\lambda = (A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。<br>(2) 学习问题。已知观测序列$O=(o_1,o_2,…,o_T)$,估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大，即用<strong>极大似然估</strong>计的方法来估计参数。<br>(3)预测问题。也称为解码(decoding)问题。已知模型$\lambda = (A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,..,i_T)$。即给定观测序列，求最优可能的对应的状态序列</p><h1 id="概率计算算法"><a href="#概率计算算法" class="headerlink" title="概率计算算法"></a>概率计算算法</h1><h2 id="直接计算法"><a href="#直接计算法" class="headerlink" title="直接计算法"></a>直接计算法</h2><p>给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算观测序列$O$的概率$P(O|\lambda)$，对所有可能的状态序列$I$求和，得到观测序列$O$的概率$P(O|\lambda)$，即</p><script type="math/tex; mode=display">\begin{aligned} P(O|\lambda) &=\sum_I P(O|I,\lambda)P(I\lambda) \newline &= \sum_{i_1,i_2,...,i_T} \pi_{i_{1}}(o_1)a_{i_{1}i_{2}}b_{i_2}(o_{2})...a_{i_{T-1}i_{T}} b_{i_{T}}(o_T) \end{aligned}</script><p>但是，利用以上的公式会产生很大的计算量，复杂度达到$O(TN^T)$阶的，使用直接计算法不行<br>那么我们既要采用前向算法或者后向算法</p><h2 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h2><p>待更新</p><h2 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h2><p>待更新</p><h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><h2 id="监督学习方法"><a href="#监督学习方法" class="headerlink" title="监督学习方法"></a>监督学习方法</h2><p>假设已给训练数据包含$S$个长度相同的观测序列和对应的状态序列${(O_1,I_1),(O_2,I_2),…,(O_s,I_s)}$，那么可以利用极大似然估计法来估计隐马尔科夫模型的参数，具体的方法如下：<br>1.转移概率$a_{ij}$的估计<br>设样本中时刻$t$处于状态$i$时刻$t+1$转移到状态$j$的频数为$A_{ij}$，那么状态转移概率$a_{ij}$的估计是：</p><script type="math/tex; mode=display">a_{ij}=frA_{ij}</script><p>待更新……</p><h1 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h1><p>待更新……</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markov </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>拉格朗日乘数法与对偶问题</title>
      <link href="/2018/07/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/"/>
      <url>/2018/07/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在机器学习的算法理论推导中，在很多地方会用到拉格朗日乘数法，在支持向量机$SVM(Support Vector Machines)           $  的学习中，我们会接触到对偶问题和KTT条件问题。于是有必要将拉格朗日乘数法与它们内在的联系弄清楚。</p><h1 id="约束最优化问题"><a href="#约束最优化问题" class="headerlink" title="约束最优化问题"></a>约束最优化问题</h1><p>假设$f(x)$，$c_i(x)$，$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题：</p><script type="math/tex; mode=display">\begin{align} \min \limits_{x \in R^n} &f(x) \newline s.t. &c_i(x) \ge0,i=1,2,...,k \newline &h_j(x)=0,j=1,2,...,l \end{align}</script><p>现在如果不考虑约束条件，原始问题就是：</p><script type="math/tex; mode=display">\min \limits_{x \in R^n}f(x)</script><p>如果其连续可微，利用高等数学的知识，对求导数，然后令导数为0，就可解出最优解，如果加上约束条件，那么就要考虑到使用条件极值来求解，拉格朗日乘数法就是来求解这种约束的极值的。<br>引进广义拉格朗日函数（generalized Lagrange function）:</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><p>其中 $x=(x^{(1)},x^{(2)},…,x^{(n)})^T \in R^n , \alpha_i,\beta_j$是拉格朗日乘子，并且 $\alpha_i \ge 0$<br>现在，我们知道$L(x,\alpha,\beta)$是关于$\alpha_i,\beta_j$的函数，那么通过确定$\alpha_i,\beta_j$的值使得$L(x,\alpha,\beta)$取得最大值(当然在求解的过程中$x$是常量)，确定了$\alpha_i,\beta_i$的值，就可以得到$L(x,\alpha,\beta)$的最大值。当$\alpha_i,\beta_i$确定时，$\max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta)$ 就只和$x$有关的函数了，定义这个函数为：</p><script type="math/tex; mode=display">\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}L(x,\alpha,\beta)</script><p>其中：</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><p>下面通过$x$是否满足约束条件，通过两个层面来进行分析这个函数：</p><ul><li>考虑到某个$x$违反了原始的约束，即$c_i(x)&gt;0$或者$h_j(x) \not= 0$，那么：<script type="math/tex; mode=display">\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}[f(x)+\sum_{i =1}^k \alpha_i c_j(x)+\sum_{j=1}^l \beta_jh_j(x)] = +\infty</script>注意中间最大化的式子就是确定$\alpha_i,\beta_j$的之后的结果，若$c_i(x) &gt; 0$，则令$\alpha_i \to +\infty$，如果$h_j(x) \not= 0$，很容易取值$\beta_j$使得$\beta_jh_j(x) \to +\infty$</li><li>考虑$x$，满足原来的约束，则$\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}[f(x)] = f(x)$，注意中间的最大化是确定$\alpha_i,\beta_j$的过程，$f(x)$就是个常量，常量的最大值就是本身<br>通过上面可以得出：<script type="math/tex; mode=display">\begin{eqnarray}f(x)=\begin{cases}f(x), &x满足原始问题的约束  \cr +\infty,&其他\end{cases}\end{eqnarray}</script>那么在满足约束条件下：<script type="math/tex; mode=display">\min \limits_{x} \theta_p(x) = \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \min \limits_{x}f(x)</script>即$\min \limits_{x} \theta_p(x)$与原始的优化问题等价，所以常用$\min \limits_{x} \theta_p(x)$代表原始问题，下表p表示原始问题，定义原始问题的最优值：<script type="math/tex; mode=display">\hat{p} = \min \limits_{x} \theta_p(x)</script></li></ul><h1 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h1><p>定义关于$\alpha,\beta$的函数：</p><script type="math/tex; mode=display">\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta)</script><p>等式的右边是关于$x$的函数的最小化，$x$确定后，最小值就只与$\alpha,\beta$有关了，所以是一个关于$\alpha,\beta$的函数。<br>考虑到极大化$\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta)$，即：</p><script type="math/tex; mode=display">\max \limits_{\alpha,\beta:\alpha_i \ge 0} \theta_D(\alpha,\beta) = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta)</script><p>这个就是原始问题的对偶问题，那么原始问题的表达式：</p><script type="math/tex; mode=display">\min \limits_{x} \theta_p(x) = \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta)</script><p>从形式上来看很对称，只不过原始问题是先固定的$L(x,\alpha,\beta)$中的$x$，优化出的参数$\alpha,\beta$，在优化最优$x$，而对偶问题是先固定$\alpha,\beta$，优化出最优的$x$，然后确定参数$\alpha,\beta$<br>定义对偶问题的最优值：</p><script type="math/tex; mode=display">\hat{d}= \max \limits_{\alpha,\beta:\alpha_i \ge 0}\theta_D(\alpha,\beta)</script><h1 id="原始问题和对偶问题的关系"><a href="#原始问题和对偶问题的关系" class="headerlink" title="原始问题和对偶问题的关系"></a>原始问题和对偶问题的关系</h1><blockquote><p><strong>定理</strong>：若原始问题与对偶问题都有最优值，则</p><script type="math/tex; mode=display">\begin{aligned} \hat{d} & = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta) \newline &\le \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \hat{p}\end{aligned}</script></blockquote><p>证明：对任意的$\alpha,\beta$和$x$，有：</p><script type="math/tex; mode=display">\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta) \le L(x,\alpha,\beta) \\ \le \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \min \limits_{x} \theta_p(x)</script><p>即：$\theta_D(\alpha,\beta) \le  \theta_p(x)$<br>由于原始问题与对偶问题都有最优值，所以：$\max \limits_{\alpha,\beta:\alpha_i \ge 0}\theta_D(\alpha,\beta) \le  \min \limits_{x}\theta_p(x)$<br>即：$\hat{d} = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta) \le \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \hat{p} $<br>也就是说原始问题的最优值不小于对偶问题的最优值，但是我们要通过对偶问题来求解原始问题，就必须使得原始问题的最优值与对偶问题的最优值相等，于是可以得出下面的推论：</p><blockquote><p><strong> 推论</strong>：设$\hat{x}$和$\hat{\alpha}，\hat{\beta}$分别是原始问题和对偶问题的可行解，如果$\hat{d}$ ，那么$\hat{x}$和$\hat{\alpha},\hat{\beta}$分别是原始问题和对偶问题的最优解。</p></blockquote><p>所以，当原始问题和对偶问题的最优值相等：$\hat{d}=\hat{p}$ ，可以用求解对偶问题来求解原始问题（当然是对偶问题求解比直接求解原始问题简单的情况下），但是到底满足什么样的条件才能使得$\hat{d}=\hat{p}$，这就是KTT条件的来源</p><h1 id="KTT条件"><a href="#KTT条件" class="headerlink" title="KTT条件"></a>KTT条件</h1><blockquote><p><strong> 定理</strong>：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数(即由一阶多项式构成的函数，$f(x) =Ax+b$，A是矩阵，$x，b$是向量)；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有的$i$有$c_i(x) &lt; 0$，则存在$\hat{x}$和$\hat{\alpha}，\hat{\beta}$，使得$\hat{x}$ 是原始问题的最优解，$\hat{\alpha}，\hat{\beta}$是对问题的最优解，并且$\hat{d}=\hat{p}=L(\hat{x},\hat{\alpha},\hat{\beta})$</p><p><strong>定理</strong>：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数(即由一阶多项式构成的函数，$f(x) =Ax+b$，$A$是矩阵，$x，b$是向量)；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有的$i$有$c_i(x) &lt; 0$，则$\hat{x}$和$\hat{\alpha},\hat{\beta}$分别是原始问题和对偶问题的最优解的充分必要条件是$\hat{x}$和$\hat{\alpha}，\hat{\beta}$，满足下面的$Karush-Kuhn-Tucker(KKT)$     条件:</p><script type="math/tex; mode=display">\begin{aligned} &\nabla _x L(\hat{x},\hat{\alpha},\hat{\beta}) =0\newline   &\nabla _{\alpha}L(\hat{x},\hat{\alpha},\hat{\beta}) =0  \newline &\nabla _{\beta} L(\hat{x},\hat{\alpha},\hat{\beta}) =0\newline &\hat{\alpha_i}c_i(\hat{x})=0, i=1,2,...,k(KTT对偶互补条件)\newline &c_i(\hat{x}) \le 0,i=1,2...,k \newline &\hat{\alpha_i} \ge 0,i=1,2,...k \newline &h_j(\hat{x}) =0,j=1,2,...,l) \end{aligned}</script></blockquote><p>关于KKT 条件的理解：前面三个条件是由解析函数的知识，对于各个变量的偏导数为0（这就解释了一开始为什么假设三个函数连续可微，如果不连续可微的话，这里的偏导数存不存在就不能保证），后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束。</p><p>特别注意当$\hat{\alpha_i}\ge 0$时，由KKT对偶互补条件可知：$c_i(\hat{x}) =0$，这个知识点会在$ SVM$ 的推导中用到.</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KTT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>EM算法原理的理解</title>
      <link href="/2018/07/12/EM%E7%AE%97%E6%B3%95/"/>
      <url>/2018/07/12/EM%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。本文就对EM算法的原理做一个总结。<br>我们经常会从样本观察数据中，找出样本的模型参数。 最常用的方法就是极大化模型分布的对数似然函数。<br>　　　　但是在一些情况下，我们得到的观察数据有未观察到的隐含数据，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。怎么办呢？这就是EM算法可以派上用场的地方了。<br>　　　　EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。<br>　　　　从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步，E步和M步。一轮轮迭代更新隐含数据和模型分布参数，直到收敛，即得到我们需要的模型参数。<br>　　　　一个最直观了解EM算法思路的是K-Means算法。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设K个初始化质心，即EM算法的E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。<br>　　　　当然，K-Means算法是比较简单的，实际中的问题往往没有这么简单。上面对EM算法的描述还很粗糙，我们需要用数学的语言精准描述。</p><h1 id="EM算法的推导"><a href="#EM算法的推导" class="headerlink" title="EM算法的推导"></a>EM算法的推导</h1><p>对于$m$个样本观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，找出样本的模型参数$θ$, 极大化模型分布的对数似然函数如下：</p><script type="math/tex; mode=display">\begin{align}L(\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m logP(x^{(i)}|\theta)\end{align}</script><p>如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},…z^{(m)})$，此时我们的极大化模型分布的对数似然函数如下：</p><script type="math/tex; mode=display">\begin{align}L(\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m logP(x^{(i)}|\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}， z^{(i)}|\theta)\end{align}</script><p>上面这个式子是没有 办法直接求出$θ$的。因此需要一些特殊的技巧， 我们首先对这个式子进行缩放如下：</p><script type="math/tex; mode=display">\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}， z^{(i)}|\theta)   & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} \\  \end{align} .......(1)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & & & & & &\geq  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} \end{align}........(2)</script><p>　上面第(1)式引入了一个未知的新的分布$Q_i(z^{(i)})$，第(2)式用到了$Jensen$不等式：<script type="math/tex">\begin{align} log\sum\limits_j\lambda_jy_j \geq \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geq 0, \sum\limits_j\lambda_j =1\end{align}</script><br>或者说由于对数函数是凹函数，所以有:</p><script type="math/tex; mode=display">f(E(x)) \geq E(f(x))\;\;如果f(x) 是凹函数</script><p>此时如果要满足$Jensen$不等式的等号，则有：</p><script type="math/tex; mode=display">\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} =c, c为常数</script><p>　由于$Q_i(z^{(i)})$是一个分布，所以满足：</p><script type="math/tex; mode=display">\sum\limits_{z}Q_i(z^{(i)}) =1</script><p>　从上面两式，我们可以得到：</p><script type="math/tex; mode=display">Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)}|\theta)}{\sum\limits_{z}P(x^{(i)}， z^{(i)}|\theta)} =  \frac{P(x^{(i)}， z^{(i)}|\theta)}{P(x^{(i)}|\theta)} = P( z^{(i)}|x^{(i)}，\theta))</script><p>如果$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}，\theta))$, 则第(2)式是我们的包含隐藏数据的对数似然的一个下界。<br>如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：</p><script type="math/tex; mode=display">arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})}</script><p>去掉上式中为常数的部分，则我们需要极大化的对数似然下界为：</p><script type="math/tex; mode=display">\begin{align} arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}\end{align}</script><p>上式也就是我们的EM算法的M步，那E步呢？注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}$可以理解为$logP(x^{(i)}， z^{(i)}|\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望。<br>至此，我们理解了EM算法中E步和M步的具体数学含义。</p><h1 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h1><p>EM算法给出了一种很有效的最大似然估计的方法：<strong>重复地构造$L(\theta)$的下界（E步），然后最大化这个下界（M步）</strong><br>现在我们总结下EM算法的流程。<br>输入：观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，联合分布$p(x,z |\theta)$条件分布$p(z|x, \theta)$最大迭代次数$J$。<br>1) 随机初始化模型参数θ的初值$\theta^{0}$<br>2） for j  from 1 to J开始EM算法迭代：</p><blockquote><p>  a) E步：计算联合分布的条件概率期望：</p><script type="math/tex; mode=display">Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}，\theta^{j}))</script><script type="math/tex; mode=display">\begin{align} L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}\end{align}</script><p>  b) M步：极大化$L(\theta, \theta^{j})$,得到$\theta^{j+1}$</p><script type="math/tex; mode=display">\begin{align} \theta^{j+1} = arg \max \limits_{\theta}L(\theta, \theta^{j})\end{align}</script><p>  c) 如果$θ^{j+1}$已收敛，则算法结束。否则继续回到步骤a)进行E步迭代。</p></blockquote><p>输出：模型参数$θ$。</p><h1 id="EM算法的收敛性思考"><a href="#EM算法的收敛性思考" class="headerlink" title="EM算法的收敛性思考"></a>EM算法的收敛性思考</h1><p>EM算法的流程并不复杂，但是还有两个问题需要我们思考：<br>　　　　1） EM算法能保证收敛吗？<br>　　　　2） EM算法如果收敛，那么能保证收敛到全局最大值吗？　　<br>　　　　首先我们来看第一个问题, EM算法的收敛性。要证明EM算法收敛，则我们需要证明我们的对数似然函数的值在迭代的过程中一直在增大(<code>说白了就证明单调性</code>)。即：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1}) \geq \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j})</script><p>由于</p><script type="math/tex; mode=display">L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j}))log{P(x^{(i)}， z^{(i)}|\theta)}</script><p>　令：<script type="math/tex">H(\theta, \theta^{j}) =  \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j}))log{P( z^{(i)}|x^{(i)}，\theta)}</script><br>上两式相减得到：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta) = L(\theta, \theta^{j}) - H(\theta, \theta^{j})</script><p>在上式中分别取$θ$为$θ^j$和$θ^{j+1}$并相减得到：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1})  - \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j}) = [L(\theta^{j+1}, \theta^{j}) - L(\theta^{j}, \theta^{j}) ] -[H(\theta^{j+1}, \theta^{j}) - H(\theta^{j}, \theta^{j}) ]</script><p>要证明EM算法的收敛性，我们只需要证明上式的右边是非负的即可。<br>由于$θ^{j+1}$使得$L(\theta, \theta^{j})$极大，因此有:</p><script type="math/tex; mode=display">L(\theta^{j+1}, \theta^{j}) - L(\theta^{j}, \theta^{j})  \geq 0</script><p>而对于第二部分，我们有：</p><script type="math/tex; mode=display">\begin{align} H(\theta^{j+1}, \theta^{j}) - H(\theta^{j}, \theta^{j})  & = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j})log\frac{P( z^{(i)}|x^{(i)}，\theta^{j+1})}{P( z^{(i)}|x^{(i)}，\theta^j)} \\ \end{align}...... (4)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & & & & & & \leq  \sum\limits_{i=1}^mlog(\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j})\frac{P( z^{(i)}|x^{(i)}，\theta^{j+1})}{P( z^{(i)}|x^{(i)}，\theta^j)}) \\ \end{align} ...... (5)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & &  = \sum\limits_{i=1}^mlog(\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j+1})) = 0  \end{align}...... (6)</script><p>其中第（4）式用到了$Jensen$不等式，只不过和第二节的使用相反而已，第（5）式用到了概率分布累积为1的性质。<br>　　　　至此，我们得到了：$\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1})  - \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j})  \geq 0$, 证明了EM算法的收敛性。<br>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。至此我们也回答了上面提到的第二个问题。</p><h1 id="EM的一点思考"><a href="#EM的一点思考" class="headerlink" title="EM的一点思考"></a>EM的一点思考</h1><p>所谓EM算法就是在含有隐变量的时候，把隐变量的分布设定为一个以观测变量为前提条件的后验分布，使得参数的似然函数与其下界相等，通过极大化这个下界来极大化似然函数，从避免直接极大化似然函数过程中因为隐变量未知而带来的困难！<code>EM算法主要是两步,E步选择出合适的隐变量分布（一个以观测变量为前提条件的后验分布），使得参数的似然函数与其下界相等；M步：极大化似然函数的下界，拟合出参数</code>.</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Adaboost的原理的理解</title>
      <link href="/2018/07/09/Adaboosting/"/>
      <url>/2018/07/09/Adaboosting/</url>
      <content type="html"><![CDATA[<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;假设给出样本的形式为 \( X =(x_1,x_2,\cdots,x_d) \),  每个样本对应一个决策值 \( Y=y \),那么可以定义一个实例\( (X,Y) = (x_1,x_2,\cdots,x_d,y)\)</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Adaboost </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于java的NIO的理解</title>
      <link href="/2018/07/07/%E5%85%B3%E4%BA%8EJavaNIO%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"/>
      <url>/2018/07/07/%E5%85%B3%E4%BA%8EJavaNIO%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/</url>
      <content type="html"><![CDATA[<h1 id="1-传统的I-O"><a href="#1-传统的I-O" class="headerlink" title="1.传统的I/O"></a>1.传统的I/O</h1><p>&nbsp;&nbsp;使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socket), 如下程序:<br>File.read(fileDesc, buf, len);<br>Socket.send(socket, buf, len);<br>会有较大的性能开销, 主要表现在一下两方面:  </p><ol><li>上下文切换(context switch), 此处有4次用户态和内核态的切换  </li><li>Buffer内存开销, 一个是应用程序buffer, 另一个是系统读取buffer以及socket buffer<br>其运行示意图如下:  </li></ol><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/02/NIO/001.png" alt="git_image">  </p><p>&nbsp;&nbsp;1) 先将文件内容从磁盘中拷贝到操作系统buffer.<br>&nbsp;&nbsp;2) 再从操作系统buffer拷贝到程序应用buffer.<br>&nbsp;&nbsp;3) 从程序buffer拷贝到socket buffer.<br>&nbsp;&nbsp;4) 从socket buffer拷贝到协议引擎.  </p><h1 id="2-NIO"><a href="#2-NIO" class="headerlink" title="2. NIO"></a>2. NIO</h1><p>&nbsp;&nbsp;NIO技术省去了将操作系统的read buffer拷贝到程序的buffer, 以及从程序buffer拷贝到socket buffer的步骤, 直接将 read buffer 拷贝到 socket buffer. java 的 FileChannel.transferTo() 方法就是这样的实现, 这个实现是依赖于操作系统底层的sendFile()实现的.<br>publicvoid transferTo(long position, long count, WritableByteChannel target);<br>他的底层调用的是系统调用sendFile()方法<br>sendfile(int out_fd, int in_fd, off_t *offset, size_t count);<br>如下图:<br><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/02/NIO/002.png" alt="git_image"></p>]]></content>
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NIO </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何使用github搭建个人博客</title>
      <link href="/2018/07/03/github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/07/03/github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<h2 id="安装git和注册github账号"><a href="#安装git和注册github账号" class="headerlink" title="安装git和注册github账号"></a>安装git和注册github账号</h2><h3 id="第一步：安装下载git"><a href="#第一步：安装下载git" class="headerlink" title="第一步：安装下载git"></a>第一步：安装下载git</h3><ul><li><p>下载<a href="https://git-for-windows.github.io/" target="_blank" rel="noopener">git</a></p></li><li><p>安装git及步骤</p></li><li>全选<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-4da9b2db5ad099b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/519" alt="git_image"></div></li><li>选择圈住的部分<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-44bddccbb0bc44fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/483" alt="git_image"></div></li></ul><p><code>余下的一路默认</code></p><h3 id="第二步：安装node-js"><a href="#第二步：安装node-js" class="headerlink" title="第二步：安装node.js"></a>第二步：安装node.js</h3><ul><li><p>node.js下载 <a href="https://nodejs.org/en/" target="_blank" rel="noopener">下载地址</a></p></li><li><p>安装步骤：一路默认即可</p></li></ul><h3 id="第三步：安装Hexo"><a href="#第三步：安装Hexo" class="headerlink" title="第三步：安装Hexo"></a>第三步：安装Hexo</h3><ul><li><p>利用 npm 命令即可安装。在任意位置点击鼠标右键，选择Git Bash<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-ed306496f5c34312.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/237" alt="git_image"></div></p></li><li><p>输入命令：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo</span><br></pre></td></tr></table></figure><p><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-0b9d56643f6b27cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/648" alt="git_image"></div></p><p><code>注意：-g是指全局安装hexo。</code></p><h3 id="第四步：初始化Hexo"><a href="#第四步：初始化Hexo" class="headerlink" title="第四步：初始化Hexo"></a>第四步：初始化Hexo</h3><ul><li>创建文件夹（我的是在E盘创建的Hexo）<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-b4cfc9da3f2063b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/650" alt=""></div></li></ul><p><code>根据个人爱好创建博客文件夹</code> </p><ul><li>在Hexo文件下，右键运行Git Bash，输入命令：hexo init<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-f6ae9b7089741c89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></li></ul><p><code>这里可能时间会长些，要耐心等待</code></p><p><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-d0452912537c03e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/658" alt=""></div></p><p><code>初始化成功后生成的一些列文件</code></p><ul><li>在_config.yml,进行基础配置<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-cd5743eda172deca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/663" alt=""></div></li></ul><p>其中可以在这里浏览更多<a href="https://hexo.io/themes/" target="_blank" rel="noopener">主题</a>，然后在Hexo文件夹下 Git Bash</p><p>输入命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span>  https://github.com/iissnan/hexo-theme-next/</span><br></pre></td></tr></table></figure><p>[next为主题名字]，来获得更多主题</p><ul><li>本地浏览博客</li></ul><p>分别输入 如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>这里有更多<a href="https://segmentfault.com/a/1190000002632530" target="_blank" rel="noopener">hexo常用命令</a></p><ul><li>写文章<br>在E:\Hexo\source_posts文件下，新建.md文件就可以写文章<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-f369abde30af73e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/478" alt=""></div></li></ul><h2 id="部署到Github上"><a href="#部署到Github上" class="headerlink" title="部署到Github上"></a>部署到Github上</h2><ul><li><p>申请Github账号，（注意别忘了进行账号邮箱验证） <code>一般使用的是Google邮箱</code></p></li><li><p>new repository</p></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/1531909-8decffce7d3866b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/494" alt="git_image"></p><ul><li><p>_config.yml进行配置 </p></li><li><p>发布到Github</p></li></ul><p>输入如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/1531909-72b3c30ffbfb1210.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/274" alt="s"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-1f99441c5f2e0cfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/359" alt="s"></p><ul><li>测试访问<br>在浏览器输入：<a href="https://xxxxx.github.io/" target="_blank" rel="noopener">https://xxxxx.github.io/</a></li></ul>]]></content>
      
      <categories>
          
          <category> Github </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> node.js </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/07/03/hello-world/"/>
      <url>/2018/07/03/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
  
  
</search>
