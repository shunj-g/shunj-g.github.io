<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>通过BPTT计算RNN的梯度</title>
      <link href="/2018/08/12/%E9%80%9A%E8%BF%87BPTT%E8%AE%A1%E7%AE%97RNN%E7%9A%84%E6%A2%AF%E5%BA%A6/"/>
      <url>/2018/08/12/%E9%80%9A%E8%BF%87BPTT%E8%AE%A1%E7%AE%97RNN%E7%9A%84%E6%A2%AF%E5%BA%A6/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我们知道循环神经网络(RNN)中一些重要的设计模式包括以下几种：</p><blockquote><ol><li>每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如图 1 所示。</li><li>每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络，如图 2 所示。</li><li>隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如图 3 所示。</li></ol></blockquote><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/RNN_1.jpg" alt="git_image"></p><font size="2">图 1: 计算循环网络(将x值的输入序列映射到输出值$o$的对应序列) 训练损失的计算图。损失$L$ 衡量每个$o$ 与相应的训练目标y 的距离。当使用$softmax$输出时，我们假设$o$是未归一化的对数概率。损失L 内部计算$\hat{y} = softmax(o)$，并将其与目标y 比较。RNN输入到隐藏的连接由权重矩阵U 参数化，隐藏到隐藏的循环连接由权重矩阵$W$ 参数化以及隐藏到输出的连接由权重矩阵$V$ 参数化。式1定义了该模型中的前向传播。(左) 使用循环连接绘制的RNN 和它的损失。(右) 同一网络被视为展开的计算图，其中每个节点现在与一个特定的时间实例相关联。</font><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/RNN_2.jpg" alt="git_image"></p><font size="2">图 2: 此类RNN 的唯一循环是从输出到隐藏层的反馈连接。在每个时间步$t$，输入为$x_t$，隐藏层激活为$h(t)$，输出为$o(t)$，目标为$y(t)$，损失为$L(t)$。(左) 回路原理图。(右) 展开的计算图。这样的RNN 没有图 1 表示的RNN 那样强大（只能表示更小的函数集合）。图 1 中的RNN 可以选择将其想要的关于过去的任何信息放入隐藏表示$h$中并且将$h$传播到未来。该图中的RNN 被训练为将特定输出值放入$o$中，并且$o$是允许传播到未来的唯一信息。此处没有从$h$前向传播的直接连接。之前的$h$仅通过产生的预测间接地连接到当前。$o$通常缺乏过去的重要信息，除非它非常高维且内容丰富。这使得该图中的RNN不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化，</font><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/ML/RNN_3.jpg" alt="git_image"></p><font size="2">图 3: 关于时间展开的循环神经网络，在序列结束时具有单个输出。这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标（如此处所示），或者通过更下游模块的反向传播来获得输出$o(t)$上的梯度。</font><p>现在我们研究图 1 中RNN的前向传播公式。这个图没有指定隐藏单元的激活函数。我们假设使用双曲正切激活函数。此外，图中没有明确指定何种形式的输出和损失函数。我们假定输出是离散的，如用于预测词或字符的RNN。表示离散变量的常规方式是把输出$o$作为每个离散变量可能值的非标准化对数概率。然后，我们可以应用$softmax$函数后续处理后，获得标准化后概率的输出向量$ \hat{y}$。RNN 从特定的初始状态$h(0)$开始前向传播。从$t = 1 $到$t = \tau$的每个时间步，我们应用以下更新方程：</p><script type="math/tex; mode=display">\begin{aligned} &a(t) = b + Wh^{(t-1)} + Ux(t); ..............(1) \newline &h(t) = tanh(a(t)); ..............(2)\newline &o(t) = c + Vh(t);..............(3) \newline &\hat{y}(t) = softmax(o(t)); ..............(4) \end{aligned}</script><p>其中的参数的偏置向量$b$和$c$连同权重矩阵$U$、$V$ 和$W$，分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的连接。这个循环网络将一个输入序列映射到相同长度的输出序列。与$x$序列配对的$y$的总损失就是所有时间步的损失之和。</p><h1 id="计算RNN的梯度"><a href="#计算RNN的梯度" class="headerlink" title="计算RNN的梯度"></a>计算RNN的梯度</h1><p>计算循环神经网络的梯度是容易的。由反向传播计算得到的梯度，并结合任何通用的基于梯度的技术就可以训练RNN。<br>为了获得BPTT算法行为的一些直观理解，我们举例说明如何通过BPTT计算上述RNN公式（式(1)）的梯度。计算图的节点包括参数$U,V,W,b$和$c$，以及以$t$ 为索引的节点序列$x(t),h(t),o(t)$和$L(t)$。对于每一个节点$N$，我们需要基于$N$后面的节点的梯度，递归地计算梯度$\nabla_NL$。我们从紧接着最终损失的节点开始递归：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial L^{(t)}} = 1</script><p>在这个导数中，我们假设输出$o(t)$作为$softmax$函数的参数，我们可以从$softmax$函数可以获得关于输出概率的向量$\hat{y}$。我们也假设损失是迄今为止给定了输入后的真实目标$y(t)$的负对数似然。对于所有$i,t$，关于时间步$t$输出的梯度$\nabla_{o(t)}L$如下：</p><script type="math/tex; mode=display">(\nabla_{o^{(t)}}L)_i = \frac{\partial L}{\partial o_{i}^{(t)}}  =\frac{\partial L }{\partial L^{(t)}} \frac{\partial L^{(t)} }{\partial o_{i}^{(t)}}= \hat{y}_{i}^{(t)} - 1_{i,y^(t)}</script><p>我们从序列的末尾开始，反向进行计算。在最后的时间步$\tau$, $h^{(\tau)}$只有$o^{(\tau)}$ 作为后续节点，因此这个梯度很简单：</p><script type="math/tex; mode=display">\nabla_{o^{(t)}}L = V^T \nabla_{o^{(t)}}L</script><p>然后，我们可以从时刻$t = \tau - 1$到$t = 1$反向迭代，通过时间反向传播梯度，注意$h(t)(t &lt; \tau)$ 同时具有$o(t)$和$h(t+1)$两个后续节点。因此，它的梯度由下式计算<br><code>这个式子看起来简单但是求解起来很容易出错，因为其中嵌套着激活函数函数，是复合函数的求道过程。</code></p><script type="math/tex; mode=display">\begin{aligned} \nabla_{h(t)}L &= (\frac{\partial h^{(t+1)}}{\partial h^{(t)}})^T(\nabla_{h(t+1)}L) + (\frac{\partial o^{(t)}}{\partial h^{(t)}})^T(\nabla_{o(t)}L) \newline   &=W^T( \nabla_{h(t+1)}L)diag(1-(h^{(t+1)})^2) + V^T(\nabla_{o(t)}L)\end{aligned}</script><p>其中$diag(1-(h^{(t+1)})^2)$ 表示包含元素  $1-(h_{i}^{(t+1)})^2$的对角矩阵。这是关于时刻$t+1$与隐藏单元i 关联的双曲正切的$Jacobian$。<br>一旦获得了计算图内部节点的梯度，我们就可以得到关于参数节点的梯度。因为参数在许多时间步共享，我们必须在表示这些变量的微积分操作时谨慎对待。我们希望实现的等式使用$bprop$ 方法计算计算图中单一边对梯度的贡献。然而微积分中的$∇_Wf$ 算子，计算$W$ 对于$f$ 的贡献时将计算图中的所有边都考虑进去了。为了消除这种歧义，我们定义只在$t $时刻使用的虚拟变量$W^{(t)}$ 作为W的副本。然后，我们可以使用$∇_{W^{(t)}}$ 表示权重在时间步$t $对梯度的贡献。<br>使用这个表示，关于剩下参数的梯度可以由下式给出：</p><script type="math/tex; mode=display">\begin{aligned} &\nabla_cL = \sum_{t}(\frac{\partial o^{(t)}}{\partial c})^T \nabla_{o^{(t)}}L=  \nabla_{o^{(t)}}L, \newline &\nabla_bL = \sum_{t}(\frac{\partial h^{(t)}}{\partial b^{(t)}})^T \nabla_{h^{(t)}}L=  \sum_{t}diag(1-(h^{(t)})^2)\nabla_{h(t)}L,\newline &\nabla_VL = \sum_{t}\sum_{i}(\frac{\partial L}{\partial o_{i}^{(t)}})^T \nabla_{V}o^{(t)}=  \sum_{t}(\nabla_{o(t)}L)h^{(t)^T}, \newline & \nabla_WL = \sum_{t}\sum_{i}(\frac{\partial L}{\partial h_{i}^{(t)}})^T \nabla_{W^{(t)}}h_{i}^{(t)} =  \sum_{t}diag(1-(h^{(t)})^2)(\nabla_{h(t)}L)h^{(t)^T},\newline & \nabla_UL = \sum_{t}\sum_{i}(\frac{\partial L}{\partial h_{i}^{(t)}})^T \nabla_{U^{(t)}}h_{i}^{(t)} =  \sum_{t}diag(1-(h^{(t)})^2)(\nabla_{h(t)}L )x^{(t)^T}, \end{aligned}</script><p>因为计算图中定义的损失的任何参数都不是训练数据$x^{(t)} $的父节点，所以我们不需要计算关于它的梯度。</p><h1 id="关于梯度消失和梯度爆炸"><a href="#关于梯度消失和梯度爆炸" class="headerlink" title="关于梯度消失和梯度爆炸"></a>关于梯度消失和梯度爆炸</h1><p>在累乘的过程中，如果取sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。其实RNN的时间序列与深层神经网络很像，在较为深层的神经网络中使用sigmoid函数做激活函数也会导致反向传播时梯度消失，梯度消失就意味消失那一层的参数再也不更新，那么那一层隐层就变成了单纯的映射层，毫无意义了，所以在深层神经网络中，有时候多加神经元数量可能会比多家深度好。<br>RNN的特点本来就是能“追根溯源“利用历史数据，现在告诉我可利用的历史数据竟然是有限的，这就令人非常难受，解决“梯度消失“是非常必要的。解决“梯度消失“的方法主要有：<br>1、选取更好的激活函数<br>2、改变传播结构<br>关于第一点，一般选用ReLU函数作为激活函数，<br>ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。还有一点就是如果左侧横为0的导数有可能导致把神经元学死，不过设置合适的步长（学习旅）也可以有效避免这个问题的发生。<br>sigmoid函数还有一个缺点，Sigmoid函数输出不是零中心对称。sigmoid的输出均大于0，这就使得输出不是0均值，称为偏移现象，这将导致后一层的神经元将上一层输出的非0均值的信号作为输入。关于原点对称的输入和中心对称的输出，网络会收敛地更好。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BPTT </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ReLU激活函数</title>
      <link href="/2018/07/28/ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/2018/07/28/ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在深度神经网络中，通常使用一种叫修正线性单元(Rectified linear unit，ReLU）作为神经元的激活函数。ReLU起源于神经科学的研究：2001年，Dayan、Abott从生物学角度模拟出了脑神经元接受信号更精确的激活模型，如下图：<br><img src="https://img-blog.csdn.net/20161113151632921.png" alt="git_image"><br><img src="./_image/20161113151632921.png" alt=""><br>其中横轴是时间(ms)，纵轴是神经元的放电速率(Firing Rate)。同年，Attwell等神经科学家通过研究大脑的能量消耗过程，推测神经元的工作方式具有稀疏性和分布性；2003年Lennie等神经科学家估测大脑同时被激活的神经元只有1~4%，这进一步表明了神经元的工作稀疏性。</p><h1 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h1><p>首先，我们来看一下ReLU激活函数的形式，如下图：</p><p><img src="./_image/20161113161105403.png" alt=""><br><img src="https://img-blog.csdn.net/20161113161105403.png" alt="git_image"><br>从上图不难看出，ReLU函数其实是分段线性函数，把所有的负值都变为0，而正值不变，这种操作被成为单侧抑制。可别小看这个简单的操作，正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。其实还不一定是这样的形状。只要能起到单侧抑制的作用，无论是镜面翻转还是180度翻转，最终神经元的输出也只是相当于加上了一个常数项系数，并不影响模型的训练结果。之所以这样定，或许是为了契合生物学角度，便于我们理解吧。</p><p>那么问题来了：这种稀疏性有何作用？换句话说，我们为什么需要让神经元稀疏？当训练一个深度分类模型的时候，和目标相关的特征往往也就那么几个，因此通过ReLU实现稀疏后的模型能够更好地挖掘相关特征，拟合训练数据。</p><p>此外，相比于其它激活函数来说，<strong>ReLU有以下优势</strong>：对于线性函数而言，ReLU的表达能力更强，尤其体现在深度网络中；而对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此<strong>不存在梯度消失问题(Vanishing Gradient Problem)</strong>，使得模型的收敛速度维持在一个稳定状态。这里稍微描述一下什么是<strong>梯度消失问题</strong>：当梯度小于1时，预测值与真实值之间的误差每传播一层会衰减一次，如果在深层模型中使用sigmoid作为激活函数，这种现象尤为明显，将导致模型收敛停滞不前。</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ReLU </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>隐马尔科夫模型</title>
      <link href="/2018/07/15/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/07/15/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>隐马尔科夫模型是关于时序的概率模型，描述一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔科夫随机生成的状态序列，称为状态序列(state sequence)；每个状态生成一个观测，而由此产生的观测的随机序列(observation sequence).。序列的每一个位置又可以看作一个时刻。<br>隐马尔科夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定。隐马尔科夫模型形式如下：<br>设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合</p><script type="math/tex; mode=display">Q={q_1,q_2,...,q_N}，V={v_1,v_2,...,v_M}</script><p>其中，$N$是可能的状态数，$M$是可能的观测数。<br>$I$是长度为$T$的状态序列，$O$是对应的观测序列。</p><script type="math/tex; mode=display">I=(i_1,i_2,...,i_T)，O=(o_1,o_2,...,o_T)</script><p>$A$是状态转移概率矩阵：</p><script type="math/tex; mode=display">A=[a_{ij}]_{N \times M}</script><p>其中，<script type="math/tex">a_{ij}=P(i_{t+1}=q_j|i_t=q_i)，i=1,2,...,N;j=1,2,...,N</script><br>是在时刻$t$处于状态$q_i$条件下载时刻$t+1$转移到状态$q_j$的概率。<br>B 是观测概率矩阵：<script type="math/tex">B=[b_j(k)]_{N \times M}</script><br>其中，<script type="math/tex">b_j(k)=P(o_t=v_k|i_i=q_i)，k=1,2,...,M;j=1,2,...,N</script><br>是在时刻$t$处于状态$q_j$的条件下生产观测$v_k$的概率。<br>$\pi$是初试状态概率向量：<script type="math/tex">\pi=(\pi_i)</script><br>其中，<script type="math/tex">\pi_i=P(i_1=q_i)，i=1,2,..,N</script><br>是时刻$t=1$处于状态$q_i$的概率。<br>隐马尔科夫模型由初试状态概率向量$\pi$，状态转移概率矩阵$A$和观测概率矩阵$B$决定。$\pi$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔科夫模型$\lambda$可以用三元符号表示，即：<script type="math/tex">\lambda=(A,B,\pi)</script><br>$A,B,\pi$称为隐马尔科夫模型的三要素。<br>状态转移概率矩阵$A$与初始状态概率向量$\pi$确定了隐藏的马尔科夫链，生成不可观测的状态序列。观测概率矩阵$B$确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。<br>从定义来看，隐马尔科夫模型作了两个基本的假设：<br>(1) 齐次马尔科夫性的假设，即假设隐藏的马尔科夫链在任意时刻$t$的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻$t$无关：</p><script type="math/tex; mode=display">P(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1})，t=1,2,...,T</script><p>(2) 观测独立性假设，即假设任意时刻的患侧只依赖于该时刻的马尔科夫链的状态，与其他观测状态无关。</p><script type="math/tex; mode=display">P(o_t|i_T,o_T,i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},...,i_1,o_1)=P(o_i|i_i)</script><h1 id="观测序列生成过程过程"><a href="#观测序列生成过程过程" class="headerlink" title="观测序列生成过程过程"></a>观测序列生成过程过程</h1><p>根据马尔科夫模型的定义，可以将一个长度为$T$的观测序列$O=(o_1,o_2,…,o_T)$的生成过程描述如下：</p><blockquote><p><strong>算法</strong> 观测序列形成<br>输入：隐马尔科夫模型$\lambda=(A,B,\pi)$，观测序列长度T<br>输出：观测序列$O=(o_1,o_2,…,o_T)$。<br>(1) 按照初始状态分布$\pi$产生状态$i_i$<br>(2) 令$t=1$<br>(3) 按照状态$i_t$的观测概率分布$b_{i_{t}}(k)$生成$o_t$<br>(4) 按照状态$i_t$的状态转移概率分布$\{a_{i_t,i_{t+1}}\}$产生状态$i_{t+1}$，$i_{t+1}=1,2,…,N$<br>(5) 令$t=t+1$;如果$t&lt;T$，转步(3);否则，终止</p></blockquote><h1 id="隐马尔科夫模型的三个基本问题"><a href="#隐马尔科夫模型的三个基本问题" class="headerlink" title="隐马尔科夫模型的三个基本问题"></a>隐马尔科夫模型的三个基本问题</h1><p>隐马尔科夫模型有3个基本问题：<br>(1) 概率计算问题，给定模型$\lambda = (A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。<br>(2) 学习问题。已知观测序列$O=(o_1,o_2,…,o_T)$,估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大，即用<strong>极大似然估</strong>计的方法来估计参数。<br>(3)预测问题。也称为解码(decoding)问题。已知模型$\lambda = (A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,..,i_T)$。即给定观测序列，求最优可能的对应的状态序列</p><h1 id="概率计算算法"><a href="#概率计算算法" class="headerlink" title="概率计算算法"></a>概率计算算法</h1><h2 id="直接计算法"><a href="#直接计算法" class="headerlink" title="直接计算法"></a>直接计算法</h2><p>给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算观测序列$O$的概率$P(O|\lambda)$，对所有可能的状态序列$I$求和，得到观测序列$O$的概率$P(O|\lambda)$，即</p><script type="math/tex; mode=display">\begin{aligned} P(O|\lambda) &=\sum_I P(O|I,\lambda)P(I\lambda) \newline &= \sum_{i_1,i_2,...,i_T} \pi_{i_{1}}(o_1)a_{i_{1}i_{2}}b_{i_2}(o_{2})...a_{i_{T-1}i_{T}} b_{i_{T}}(o_T) \end{aligned}</script><p>但是，利用以上的公式会产生很大的计算量，复杂度达到$O(TN^T)$阶的，使用直接计算法不行<br>那么我们既要采用前向算法或者后向算法</p><h2 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h2><p>待更新</p><h2 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h2><p>待更新</p><h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><h2 id="监督学习方法"><a href="#监督学习方法" class="headerlink" title="监督学习方法"></a>监督学习方法</h2><p>假设已给训练数据包含$S$个长度相同的观测序列和对应的状态序列${(O_1,I_1),(O_2,I_2),…,(O_s,I_s)}$，那么可以利用极大似然估计法来估计隐马尔科夫模型的参数，具体的方法如下：<br>1.转移概率$a_{ij}$的估计<br>设样本中时刻$t$处于状态$i$时刻$t+1$转移到状态$j$的频数为$A_{ij}$，那么状态转移概率$a_{ij}$的估计是：</p><script type="math/tex; mode=display">a_{ij}=frA_{ij}</script><p>待更新……</p><h1 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h1><p>待更新……</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markov </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>拉格朗日乘数法与对偶问题</title>
      <link href="/2018/07/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/"/>
      <url>/2018/07/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在机器学习的算法理论推导中，在很多地方会用到拉格朗日乘数法，在支持向量机$SVM(Support Vector Machines)           $  的学习中，我们会接触到对偶问题和KTT条件问题。于是有必要将拉格朗日乘数法与它们内在的联系弄清楚。</p><h1 id="约束最优化问题"><a href="#约束最优化问题" class="headerlink" title="约束最优化问题"></a>约束最优化问题</h1><p>假设$f(x)$，$c_i(x)$，$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题：</p><script type="math/tex; mode=display">\begin{align} \min \limits_{x \in R^n} &f(x) \newline s.t. &c_i(x) \ge0,i=1,2,...,k \newline &h_j(x)=0,j=1,2,...,l \end{align}</script><p>现在如果不考虑约束条件，原始问题就是：</p><script type="math/tex; mode=display">\min \limits_{x \in R^n}f(x)</script><p>如果其连续可微，利用高等数学的知识，对求导数，然后令导数为0，就可解出最优解，如果加上约束条件，那么就要考虑到使用条件极值来求解，拉格朗日乘数法就是来求解这种约束的极值的。<br>引进广义拉格朗日函数（generalized Lagrange function）:</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><p>其中 $x=(x^{(1)},x^{(2)},…,x^{(n)})^T \in R^n , \alpha_i,\beta_j$是拉格朗日乘子，并且 $\alpha_i \ge 0$<br>现在，我们知道$L(x,\alpha,\beta)$是关于$\alpha_i,\beta_j$的函数，那么通过确定$\alpha_i,\beta_j$的值使得$L(x,\alpha,\beta)$取得最大值(当然在求解的过程中$x$是常量)，确定了$\alpha_i,\beta_i$的值，就可以得到$L(x,\alpha,\beta)$的最大值。当$\alpha_i,\beta_i$确定时，$\max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta)$ 就只和$x$有关的函数了，定义这个函数为：</p><script type="math/tex; mode=display">\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}L(x,\alpha,\beta)</script><p>其中：</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><p>下面通过$x$是否满足约束条件，通过两个层面来进行分析这个函数：</p><ul><li>考虑到某个$x$违反了原始的约束，即$c_i(x)&gt;0$或者$h_j(x) \not= 0$，那么：<script type="math/tex; mode=display">\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}[f(x)+\sum_{i =1}^k \alpha_i c_j(x)+\sum_{j=1}^l \beta_jh_j(x)] = +\infty</script>注意中间最大化的式子就是确定$\alpha_i,\beta_j$的之后的结果，若$c_i(x) &gt; 0$，则令$\alpha_i \to +\infty$，如果$h_j(x) \not= 0$，很容易取值$\beta_j$使得$\beta_jh_j(x) \to +\infty$</li><li>考虑$x$，满足原来的约束，则$\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}[f(x)] = f(x)$，注意中间的最大化是确定$\alpha_i,\beta_j$的过程，$f(x)$就是个常量，常量的最大值就是本身<br>通过上面可以得出：<script type="math/tex; mode=display">\begin{eqnarray}f(x)=\begin{cases}f(x), &x满足原始问题的约束  \cr +\infty,&其他\end{cases}\end{eqnarray}</script>那么在满足约束条件下：<script type="math/tex; mode=display">\min \limits_{x} \theta_p(x) = \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \min \limits_{x}f(x)</script>即$\min \limits_{x} \theta_p(x)$与原始的优化问题等价，所以常用$\min \limits_{x} \theta_p(x)$代表原始问题，下表p表示原始问题，定义原始问题的最优值：<script type="math/tex; mode=display">\hat{p} = \min \limits_{x} \theta_p(x)</script></li></ul><h1 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h1><p>定义关于$\alpha,\beta$的函数：</p><script type="math/tex; mode=display">\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta)</script><p>等式的右边是关于$x$的函数的最小化，$x$确定后，最小值就只与$\alpha,\beta$有关了，所以是一个关于$\alpha,\beta$的函数。<br>考虑到极大化$\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta)$，即：</p><script type="math/tex; mode=display">\max \limits_{\alpha,\beta:\alpha_i \ge 0} \theta_D(\alpha,\beta) = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta)</script><p>这个就是原始问题的对偶问题，那么原始问题的表达式：</p><script type="math/tex; mode=display">\min \limits_{x} \theta_p(x) = \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta)</script><p>从形式上来看很对称，只不过原始问题是先固定的$L(x,\alpha,\beta)$中的$x$，优化出的参数$\alpha,\beta$，在优化最优$x$，而对偶问题是先固定$\alpha,\beta$，优化出最优的$x$，然后确定参数$\alpha,\beta$<br>定义对偶问题的最优值：</p><script type="math/tex; mode=display">\hat{d}= \max \limits_{\alpha,\beta:\alpha_i \ge 0}\theta_D(\alpha,\beta)</script><h1 id="原始问题和对偶问题的关系"><a href="#原始问题和对偶问题的关系" class="headerlink" title="原始问题和对偶问题的关系"></a>原始问题和对偶问题的关系</h1><blockquote><p><strong>定理</strong>：若原始问题与对偶问题都有最优值，则</p><script type="math/tex; mode=display">\begin{aligned} \hat{d} & = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta) \newline &\le \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \hat{p}\end{aligned}</script></blockquote><p>证明：对任意的$\alpha,\beta$和$x$，有：</p><script type="math/tex; mode=display">\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta) \le L(x,\alpha,\beta) \\ \le \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \min \limits_{x} \theta_p(x)</script><p>即：$\theta_D(\alpha,\beta) \le  \theta_p(x)$<br>由于原始问题与对偶问题都有最优值，所以：$\max \limits_{\alpha,\beta:\alpha_i \ge 0}\theta_D(\alpha,\beta) \le  \min \limits_{x}\theta_p(x)$<br>即：$\hat{d} = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta) \le \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \hat{p} $<br>也就是说原始问题的最优值不小于对偶问题的最优值，但是我们要通过对偶问题来求解原始问题，就必须使得原始问题的最优值与对偶问题的最优值相等，于是可以得出下面的推论：</p><blockquote><p><strong> 推论</strong>：设$\hat{x}$和$\hat{\alpha}，\hat{\beta}$分别是原始问题和对偶问题的可行解，如果$\hat{d}$ ，那么$\hat{x}$和$\hat{\alpha},\hat{\beta}$分别是原始问题和对偶问题的最优解。</p></blockquote><p>所以，当原始问题和对偶问题的最优值相等：$\hat{d}=\hat{p}$ ，可以用求解对偶问题来求解原始问题（当然是对偶问题求解比直接求解原始问题简单的情况下），但是到底满足什么样的条件才能使得$\hat{d}=\hat{p}$，这就是KTT条件的来源</p><h1 id="KTT条件"><a href="#KTT条件" class="headerlink" title="KTT条件"></a>KTT条件</h1><blockquote><p><strong> 定理</strong>：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数(即由一阶多项式构成的函数，$f(x) =Ax+b$，A是矩阵，$x，b$是向量)；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有的$i$有$c_i(x) &lt; 0$，则存在$\hat{x}$和$\hat{\alpha}，\hat{\beta}$，使得$\hat{x}$ 是原始问题的最优解，$\hat{\alpha}，\hat{\beta}$是对问题的最优解，并且$\hat{d}=\hat{p}=L(\hat{x},\hat{\alpha},\hat{\beta})$</p><p><strong>定理</strong>：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数(即由一阶多项式构成的函数，$f(x) =Ax+b$，$A$是矩阵，$x，b$是向量)；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有的$i$有$c_i(x) &lt; 0$，则$\hat{x}$和$\hat{\alpha},\hat{\beta}$分别是原始问题和对偶问题的最优解的充分必要条件是$\hat{x}$和$\hat{\alpha}，\hat{\beta}$，满足下面的$Karush-Kuhn-Tucker(KKT)$     条件:</p><script type="math/tex; mode=display">\begin{aligned} &\nabla _x L(\hat{x},\hat{\alpha},\hat{\beta}) =0\newline   &\nabla _{\alpha}L(\hat{x},\hat{\alpha},\hat{\beta}) =0  \newline &\nabla _{\beta} L(\hat{x},\hat{\alpha},\hat{\beta}) =0\newline &\hat{\alpha_i}c_i(\hat{x})=0, i=1,2,...,k(KTT对偶互补条件)\newline &c_i(\hat{x}) \le 0,i=1,2...,k \newline &\hat{\alpha_i} \ge 0,i=1,2,...k \newline &h_j(\hat{x}) =0,j=1,2,...,l) \end{aligned}</script></blockquote><p>关于KKT 条件的理解：前面三个条件是由解析函数的知识，对于各个变量的偏导数为0（这就解释了一开始为什么假设三个函数连续可微，如果不连续可微的话，这里的偏导数存不存在就不能保证），后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束。</p><p>特别注意当$\hat{\alpha_i}\ge 0$时，由KKT对偶互补条件可知：$c_i(\hat{x}) =0$，这个知识点会在$ SVM$ 的推导中用到.</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KTT </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>EM算法原理的理解</title>
      <link href="/2018/07/12/EM%E7%AE%97%E6%B3%95/"/>
      <url>/2018/07/12/EM%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。本文就对EM算法的原理做一个总结。<br>我们经常会从样本观察数据中，找出样本的模型参数。 最常用的方法就是极大化模型分布的对数似然函数。<br>　　　　但是在一些情况下，我们得到的观察数据有未观察到的隐含数据，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。怎么办呢？这就是EM算法可以派上用场的地方了。<br>　　　　EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。<br>　　　　从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步，E步和M步。一轮轮迭代更新隐含数据和模型分布参数，直到收敛，即得到我们需要的模型参数。<br>　　　　一个最直观了解EM算法思路的是K-Means算法。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设K个初始化质心，即EM算法的E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。<br>　　　　当然，K-Means算法是比较简单的，实际中的问题往往没有这么简单。上面对EM算法的描述还很粗糙，我们需要用数学的语言精准描述。</p><h1 id="EM算法的推导"><a href="#EM算法的推导" class="headerlink" title="EM算法的推导"></a>EM算法的推导</h1><p>对于$m$个样本观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，找出样本的模型参数$θ$, 极大化模型分布的对数似然函数如下：</p><script type="math/tex; mode=display">\begin{align}L(\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m logP(x^{(i)}|\theta)\end{align}</script><p>如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},…z^{(m)})$，此时我们的极大化模型分布的对数似然函数如下：</p><script type="math/tex; mode=display">\begin{align}L(\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m logP(x^{(i)}|\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}， z^{(i)}|\theta)\end{align}</script><p>上面这个式子是没有 办法直接求出$θ$的。因此需要一些特殊的技巧， 我们首先对这个式子进行缩放如下：</p><script type="math/tex; mode=display">\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}， z^{(i)}|\theta)   & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} \\  \end{align} .......(1)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & & & & & &\geq  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} \end{align}........(2)</script><p>　上面第(1)式引入了一个未知的新的分布$Q_i(z^{(i)})$，第(2)式用到了$Jensen$不等式：<script type="math/tex">\begin{align} log\sum\limits_j\lambda_jy_j \geq \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geq 0, \sum\limits_j\lambda_j =1\end{align}</script><br>或者说由于对数函数是凹函数，所以有:</p><script type="math/tex; mode=display">f(E(x)) \geq E(f(x))\;\;如果f(x) 是凹函数</script><p>此时如果要满足$Jensen$不等式的等号，则有：</p><script type="math/tex; mode=display">\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} =c, c为常数</script><p>　由于$Q_i(z^{(i)})$是一个分布，所以满足：</p><script type="math/tex; mode=display">\sum\limits_{z}Q_i(z^{(i)}) =1</script><p>　从上面两式，我们可以得到：</p><script type="math/tex; mode=display">Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)}|\theta)}{\sum\limits_{z}P(x^{(i)}， z^{(i)}|\theta)} =  \frac{P(x^{(i)}， z^{(i)}|\theta)}{P(x^{(i)}|\theta)} = P( z^{(i)}|x^{(i)}，\theta))</script><p>如果$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}，\theta))$, 则第(2)式是我们的包含隐藏数据的对数似然的一个下界。<br>如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：</p><script type="math/tex; mode=display">arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})}</script><p>去掉上式中为常数的部分，则我们需要极大化的对数似然下界为：</p><script type="math/tex; mode=display">\begin{align} arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}\end{align}</script><p>上式也就是我们的EM算法的M步，那E步呢？注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}$可以理解为$logP(x^{(i)}， z^{(i)}|\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望。<br>至此，我们理解了EM算法中E步和M步的具体数学含义。</p><h1 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h1><p>EM算法给出了一种很有效的最大似然估计的方法：<strong>重复地构造$L(\theta)$的下界（E步），然后最大化这个下界（M步）</strong><br>现在我们总结下EM算法的流程。<br>输入：观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，联合分布$p(x,z |\theta)$条件分布$p(z|x, \theta)$最大迭代次数$J$。<br>1) 随机初始化模型参数θ的初值$\theta^{0}$<br>2） for j  from 1 to J开始EM算法迭代：</p><blockquote><p>  a) E步：计算联合分布的条件概率期望：</p><script type="math/tex; mode=display">Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}，\theta^{j}))</script><script type="math/tex; mode=display">\begin{align} L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}\end{align}</script><p>  b) M步：极大化$L(\theta, \theta^{j})$,得到$\theta^{j+1}$</p><script type="math/tex; mode=display">\begin{align} \theta^{j+1} = arg \max \limits_{\theta}L(\theta, \theta^{j})\end{align}</script><p>  c) 如果$θ^{j+1}$已收敛，则算法结束。否则继续回到步骤a)进行E步迭代。</p></blockquote><p>输出：模型参数$θ$。</p><h1 id="EM算法的收敛性思考"><a href="#EM算法的收敛性思考" class="headerlink" title="EM算法的收敛性思考"></a>EM算法的收敛性思考</h1><p>EM算法的流程并不复杂，但是还有两个问题需要我们思考：<br>　　　　1） EM算法能保证收敛吗？<br>　　　　2） EM算法如果收敛，那么能保证收敛到全局最大值吗？　　<br>　　　　首先我们来看第一个问题, EM算法的收敛性。要证明EM算法收敛，则我们需要证明我们的对数似然函数的值在迭代的过程中一直在增大(<code>说白了就证明单调性</code>)。即：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1}) \geq \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j})</script><p>由于</p><script type="math/tex; mode=display">L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j}))log{P(x^{(i)}， z^{(i)}|\theta)}</script><p>　令：<script type="math/tex">H(\theta, \theta^{j}) =  \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j}))log{P( z^{(i)}|x^{(i)}，\theta)}</script><br>上两式相减得到：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta) = L(\theta, \theta^{j}) - H(\theta, \theta^{j})</script><p>在上式中分别取$θ$为$θ^j$和$θ^{j+1}$并相减得到：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1})  - \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j}) = [L(\theta^{j+1}, \theta^{j}) - L(\theta^{j}, \theta^{j}) ] -[H(\theta^{j+1}, \theta^{j}) - H(\theta^{j}, \theta^{j}) ]</script><p>要证明EM算法的收敛性，我们只需要证明上式的右边是非负的即可。<br>由于$θ^{j+1}$使得$L(\theta, \theta^{j})$极大，因此有:</p><script type="math/tex; mode=display">L(\theta^{j+1}, \theta^{j}) - L(\theta^{j}, \theta^{j})  \geq 0</script><p>而对于第二部分，我们有：</p><script type="math/tex; mode=display">\begin{align} H(\theta^{j+1}, \theta^{j}) - H(\theta^{j}, \theta^{j})  & = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j})log\frac{P( z^{(i)}|x^{(i)}，\theta^{j+1})}{P( z^{(i)}|x^{(i)}，\theta^j)} \\ \end{align}...... (4)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & & & & & & \leq  \sum\limits_{i=1}^mlog(\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j})\frac{P( z^{(i)}|x^{(i)}，\theta^{j+1})}{P( z^{(i)}|x^{(i)}，\theta^j)}) \\ \end{align} ...... (5)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & &  = \sum\limits_{i=1}^mlog(\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j+1})) = 0  \end{align}...... (6)</script><p>其中第（4）式用到了$Jensen$不等式，只不过和第二节的使用相反而已，第（5）式用到了概率分布累积为1的性质。<br>　　　　至此，我们得到了：$\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1})  - \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j})  \geq 0$, 证明了EM算法的收敛性。<br>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。至此我们也回答了上面提到的第二个问题。</p><h1 id="EM的一点思考"><a href="#EM的一点思考" class="headerlink" title="EM的一点思考"></a>EM的一点思考</h1><p>所谓EM算法就是在含有隐变量的时候，把隐变量的分布设定为一个以观测变量为前提条件的后验分布，使得参数的似然函数与其下界相等，通过极大化这个下界来极大化似然函数，从避免直接极大化似然函数过程中因为隐变量未知而带来的困难！<code>EM算法主要是两步,E步选择出合适的隐变量分布（一个以观测变量为前提条件的后验分布），使得参数的似然函数与其下界相等；M步：极大化似然函数的下界，拟合出参数</code>.</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于Adaboost的原理的理解</title>
      <link href="/2018/07/09/Adaboosting/"/>
      <url>/2018/07/09/Adaboosting/</url>
      <content type="html"><![CDATA[<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;假设给出样本的形式为 \( X =(x_1,x_2,\cdots,x_d) \),  每个样本对应一个决策值 \( Y=y \),那么可以定义一个实例\( (X,Y) = (x_1,x_2,\cdots,x_d,y)\)</p>]]></content>
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Adaboost </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>关于java的NIO的理解</title>
      <link href="/2018/07/07/%E5%85%B3%E4%BA%8EJavaNIO%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"/>
      <url>/2018/07/07/%E5%85%B3%E4%BA%8EJavaNIO%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/</url>
      <content type="html"><![CDATA[<h1 id="1-传统的I-O"><a href="#1-传统的I-O" class="headerlink" title="1.传统的I/O"></a>1.传统的I/O</h1><p>&nbsp;&nbsp;使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socket), 如下程序:<br>File.read(fileDesc, buf, len);<br>Socket.send(socket, buf, len);<br>会有较大的性能开销, 主要表现在一下两方面:  </p><ol><li>上下文切换(context switch), 此处有4次用户态和内核态的切换  </li><li>Buffer内存开销, 一个是应用程序buffer, 另一个是系统读取buffer以及socket buffer<br>其运行示意图如下:  </li></ol><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/02/NIO/001.png" alt="git_image">  </p><p>&nbsp;&nbsp;1) 先将文件内容从磁盘中拷贝到操作系统buffer.<br>&nbsp;&nbsp;2) 再从操作系统buffer拷贝到程序应用buffer.<br>&nbsp;&nbsp;3) 从程序buffer拷贝到socket buffer.<br>&nbsp;&nbsp;4) 从socket buffer拷贝到协议引擎.  </p><h1 id="2-NIO"><a href="#2-NIO" class="headerlink" title="2. NIO"></a>2. NIO</h1><p>&nbsp;&nbsp;NIO技术省去了将操作系统的read buffer拷贝到程序的buffer, 以及从程序buffer拷贝到socket buffer的步骤, 直接将 read buffer 拷贝到 socket buffer. java 的 FileChannel.transferTo() 方法就是这样的实现, 这个实现是依赖于操作系统底层的sendFile()实现的.<br>publicvoid transferTo(long position, long count, WritableByteChannel target);<br>他的底层调用的是系统调用sendFile()方法<br>sendfile(int out_fd, int in_fd, off_t *offset, size_t count);<br>如下图:<br><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/02/NIO/002.png" alt="git_image"></p>]]></content>
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NIO </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何使用github搭建个人博客</title>
      <link href="/2018/07/03/github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/07/03/github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<h2 id="安装git和注册github账号"><a href="#安装git和注册github账号" class="headerlink" title="安装git和注册github账号"></a>安装git和注册github账号</h2><h3 id="第一步：安装下载git"><a href="#第一步：安装下载git" class="headerlink" title="第一步：安装下载git"></a>第一步：安装下载git</h3><ul><li><p>下载<a href="https://git-for-windows.github.io/" target="_blank" rel="noopener">git</a></p></li><li><p>安装git及步骤</p></li><li>全选<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-4da9b2db5ad099b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/519" alt="git_image"></div></li><li>选择圈住的部分<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-44bddccbb0bc44fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/483" alt="git_image"></div></li></ul><p><code>余下的一路默认</code></p><h3 id="第二步：安装node-js"><a href="#第二步：安装node-js" class="headerlink" title="第二步：安装node.js"></a>第二步：安装node.js</h3><ul><li><p>node.js下载 <a href="https://nodejs.org/en/" target="_blank" rel="noopener">下载地址</a></p></li><li><p>安装步骤：一路默认即可</p></li></ul><h3 id="第三步：安装Hexo"><a href="#第三步：安装Hexo" class="headerlink" title="第三步：安装Hexo"></a>第三步：安装Hexo</h3><ul><li><p>利用 npm 命令即可安装。在任意位置点击鼠标右键，选择Git Bash<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-ed306496f5c34312.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/237" alt="git_image"></div></p></li><li><p>输入命令：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo</span><br></pre></td></tr></table></figure><p><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-0b9d56643f6b27cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/648" alt="git_image"></div></p><p><code>注意：-g是指全局安装hexo。</code></p><h3 id="第四步：初始化Hexo"><a href="#第四步：初始化Hexo" class="headerlink" title="第四步：初始化Hexo"></a>第四步：初始化Hexo</h3><ul><li>创建文件夹（我的是在E盘创建的Hexo）<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-b4cfc9da3f2063b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/650" alt=""></div></li></ul><p><code>根据个人爱好创建博客文件夹</code> </p><ul><li>在Hexo文件下，右键运行Git Bash，输入命令：hexo init<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-f6ae9b7089741c89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></li></ul><p><code>这里可能时间会长些，要耐心等待</code></p><p><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-d0452912537c03e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/658" alt=""></div></p><p><code>初始化成功后生成的一些列文件</code></p><ul><li>在_config.yml,进行基础配置<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-cd5743eda172deca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/663" alt=""></div></li></ul><p>其中可以在这里浏览更多<a href="https://hexo.io/themes/" target="_blank" rel="noopener">主题</a>，然后在Hexo文件夹下 Git Bash</p><p>输入命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span>  https://github.com/iissnan/hexo-theme-next/</span><br></pre></td></tr></table></figure><p>[next为主题名字]，来获得更多主题</p><ul><li>本地浏览博客</li></ul><p>分别输入 如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>这里有更多<a href="https://segmentfault.com/a/1190000002632530" target="_blank" rel="noopener">hexo常用命令</a></p><ul><li>写文章<br>在E:\Hexo\source_posts文件下，新建.md文件就可以写文章<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-f369abde30af73e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/478" alt=""></div></li></ul><h2 id="部署到Github上"><a href="#部署到Github上" class="headerlink" title="部署到Github上"></a>部署到Github上</h2><ul><li><p>申请Github账号，（注意别忘了进行账号邮箱验证） <code>一般使用的是Google邮箱</code></p></li><li><p>new repository</p></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/1531909-8decffce7d3866b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/494" alt="git_image"></p><ul><li><p>_config.yml进行配置 </p></li><li><p>发布到Github</p></li></ul><p>输入如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/1531909-72b3c30ffbfb1210.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/274" alt="s"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-1f99441c5f2e0cfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/359" alt="s"></p><ul><li>测试访问<br>在浏览器输入：<a href="https://xxxxx.github.io/" target="_blank" rel="noopener">https://xxxxx.github.io/</a></li></ul>]]></content>
      
      <categories>
          
          <category> Github </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> node.js </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/07/03/hello-world/"/>
      <url>/2018/07/03/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
  
  
</search>
