<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>我的技术博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-07-17T15:12:40.493Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>shunj-g</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>隐马尔科夫模型</title>
    <link href="http://yoursite.com/2018/07/15/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/07/15/隐马尔科夫模型/</id>
    <published>2018-07-15T08:30:21.172Z</published>
    <updated>2018-07-17T15:12:40.493Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>隐马尔科夫模型是关于时序的概率模型，描述一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔科夫随机生成的状态序列，称为状态序列(state sequence)；每个状态生成一个观测，而由此产生的观测的随机序列(observation sequence).。序列的每一个位置又可以看作一个时刻。<br>隐马尔科夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定。隐马尔科夫模型形式如下：<br>设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合</p><script type="math/tex; mode=display">Q={q_1,q_2,...,q_N}，V={v_1,v_2,...,v_M}</script><p>其中，$N$是可能的状态数，$M$是可能的观测数。<br>$I$是长度为$T$的状态序列，$O$是对应的观测序列。</p><script type="math/tex; mode=display">I=(i_1,i_2,...,i_T)，O=(o_1,o_2,...,o_T)</script><p>$A$是状态转移概率矩阵：</p><script type="math/tex; mode=display">A=[a_{ij}]_{N \times M}</script><p>其中，<script type="math/tex">a_{ij}=P(i_{t+1}=q_j|i_t=q_i)，i=1,2,...,N;j=1,2,...,N</script><br>是在时刻$t$处于状态$q_i$条件下载时刻$t+1$转移到状态$q_j$的概率。<br>B 是观测概率矩阵：<script type="math/tex">B=[b_j(k)]_{N \times M}</script><br>其中，<script type="math/tex">b_j(k)=P(o_t=v_k|i_i=q_i)，k=1,2,...,M;j=1,2,...,N</script><br>是在时刻$t$处于状态$q_j$的条件下生产观测$v_k$的概率。<br>$\pi$是初试状态概率向量：<script type="math/tex">\pi=(\pi_i)</script><br>其中，<script type="math/tex">\pi_i=P(i_1=q_i)，i=1,2,..,N</script><br>是时刻$t=1$处于状态$q_i$的概率。<br>隐马尔科夫模型由初试状态概率向量$\pi$，状态转移概率矩阵$A$和观测概率矩阵$B$决定。$\pi$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔科夫模型$\lambda$可以用三元符号表示，即：<script type="math/tex">\lambda=(A,B,\pi)</script><br>$A,B,\pi$称为隐马尔科夫模型的三要素。<br>状态转移概率矩阵$A$与初始状态概率向量$\pi$确定了隐藏的马尔科夫链，生成不可观测的状态序列。观测概率矩阵$B$确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。<br>从定义来看，隐马尔科夫模型作了两个基本的假设：<br>(1) 齐次马尔科夫性的假设，即假设隐藏的马尔科夫链在任意时刻$t$的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻$t$无关：</p><script type="math/tex; mode=display">P(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1})，t=1,2,...,T</script><p>(2) 观测独立性假设，即假设任意时刻的患侧只依赖于该时刻的马尔科夫链的状态，与其他观测状态无关。</p><script type="math/tex; mode=display">P(o_t|i_T,o_T,i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},...,i_1,o_1)=P(o_i|i_i)</script><h1 id="观测序列生成过程过程"><a href="#观测序列生成过程过程" class="headerlink" title="观测序列生成过程过程"></a>观测序列生成过程过程</h1><p>根据马尔科夫模型的定义，可以将一个长度为$T$的观测序列$O=(o_1,o_2,…,o_T)$的生成过程描述如下：</p><blockquote><p><strong>算法</strong> 观测序列形成<br>输入：隐马尔科夫模型$\lambda=(A,B,\pi)$，观测序列长度T<br>输出：观测序列$O=(o_1,o_2,…,o_T)$。<br>(1) 按照初始状态分布$\pi$产生状态$i_i$<br>(2) 令$t=1$<br>(3) 按照状态$i_t$的观测概率分布$b_{i_{t}}(k)$生成$o_t$<br>(4) 按照状态$i_t$的状态转移概率分布$\{a_{i_t,i_{t+1}}\}$产生状态$i_{t+1}$，$i_{t+1}=1,2,…,N$<br>(5) 令$t=t+1$;如果$t&lt;T$，转步(3);否则，终止</p></blockquote><h1 id="隐马尔科夫模型的三个基本问题"><a href="#隐马尔科夫模型的三个基本问题" class="headerlink" title="隐马尔科夫模型的三个基本问题"></a>隐马尔科夫模型的三个基本问题</h1><p>隐马尔科夫模型有3个基本问题：<br>(1) 概率计算问题，给定模型$\lambda = (A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。<br>(2) 学习问题。已知观测序列$O=(o_1,o_2,…,o_T)$,估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大，即用<strong>极大似然估</strong>计的方法来估计参数。<br>(3)预测问题。也称为解码(decoding)问题。已知模型$\lambda = (A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,..,i_T)$。即给定观测序列，求最优可能的对应的状态序列</p><h1 id="概率计算算法"><a href="#概率计算算法" class="headerlink" title="概率计算算法"></a>概率计算算法</h1><h2 id="直接计算法"><a href="#直接计算法" class="headerlink" title="直接计算法"></a>直接计算法</h2><p>给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算观测序列$O$的概率$P(O|\lambda)$，对所有可能的状态序列$I$求和，得到观测序列$O$的概率$P(O|\lambda)$，即<script type="math/tex">\\begin{aligned}  P(O|\lambda) =\sum_I P(O|I,\lambda)P(I\\lambda) \newline &= \sum_{i_1,i_2,...,i_T} \pi_{i_{1} b_{i_{1}}(o_1) a_{i_{1}i_{2}} b_{i_2}(o_{2}...a_{i_{T-1}i_{T}} b_{i_{T}}(o_T) \end{aligned}</script><br>但是，利用以上的公式会产生很大的计算量，复杂度达到$O(TN^T)$阶的，使用直接计算法不行<br>那么我们既要采用前向算法或者后向算法</p><h2 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h2><p>待更新</p><h2 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h2><p>待更新</p><h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><h2 id="监督学习方法"><a href="#监督学习方法" class="headerlink" title="监督学习方法"></a>监督学习方法</h2><p>假设已给训练数据包含$S$个长度相同的观测序列和对应的状态序列${(O_1,I_1),(O_2,I_2),…,(O_s,I_s)}$，那么可以利用极大似然估计法来估计隐马尔科夫模型的参数，具体的方法如下：<br>1.转移概率$a_{ij}$的估计<br>设样本中时刻$t$处于状态$i$时刻$t+1$转移到状态$j$的频数为$A_{ij}$，那么状态转移概率$a_{ij}$的估计是：</p><script type="math/tex; mode=display">a_{ij}=frA_{ij}</script><p>待更新……</p><h1 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h1><p>待更新……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;隐马尔科夫模型是关于时序的概率模型，描述一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="markov" scheme="http://yoursite.com/tags/markov/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日乘数法与对偶问题</title>
    <link href="http://yoursite.com/2018/07/13/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E4%B8%8E%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/07/13/拉格朗日乘数法与对偶问题/</id>
    <published>2018-07-13T10:40:07.398Z</published>
    <updated>2018-07-14T06:21:23.740Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在机器学习的算法理论推导中，在很多地方会用到拉格朗日乘数法，在支持向量机$SVM(Support Vector Machines)           $  的学习中，我们会接触到对偶问题和KTT条件问题。于是有必要将拉格朗日乘数法与它们内在的联系弄清楚。</p><h1 id="约束最优化问题"><a href="#约束最优化问题" class="headerlink" title="约束最优化问题"></a>约束最优化问题</h1><p>假设$f(x)$，$c_i(x)$，$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题：</p><script type="math/tex; mode=display">\begin{align} \min \limits_{x \in R^n} &f(x) \newline s.t. &c_i(x) \ge0,i=1,2,...,k \newline &h_j(x)=0,j=1,2,...,l \end{align}</script><p>现在如果不考虑约束条件，原始问题就是：</p><script type="math/tex; mode=display">\min \limits_{x \in R^n}f(x)</script><p>如果其连续可微，利用高等数学的知识，对求导数，然后令导数为0，就可解出最优解，如果加上约束条件，那么就要考虑到使用条件极值来求解，拉格朗日乘数法就是来求解这种约束的极值的。<br>引进广义拉格朗日函数（generalized Lagrange function）:</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><p>其中 $x=(x^{(1)},x^{(2)},…,x^{(n)})^T \in R^n , \alpha_i,\beta_j$是拉格朗日乘子，并且 $\alpha_i \ge 0$<br>现在，我们知道$L(x,\alpha,\beta)$是关于$\alpha_i,\beta_j$的函数，那么通过确定$\alpha_i,\beta_j$的值使得$L(x,\alpha,\beta)$取得最大值(当然在求解的过程中$x$是常量)，确定了$\alpha_i,\beta_i$的值，就可以得到$L(x,\alpha,\beta)$的最大值。当$\alpha_i,\beta_i$确定时，$\max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta)$ 就只和$x$有关的函数了，定义这个函数为：</p><script type="math/tex; mode=display">\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}L(x,\alpha,\beta)</script><p>其中：</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x)+\sum_{i=1}^k \alpha_i c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><p>下面通过$x$是否满足约束条件，通过两个层面来进行分析这个函数：</p><ul><li>考虑到某个$x$违反了原始的约束，即$c_i(x)&gt;0$或者$h_j(x) \not= 0$，那么：<script type="math/tex; mode=display">\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}[f(x)+\sum_{i =1}^k \alpha_i c_j(x)+\sum_{j=1}^l \beta_jh_j(x)] = +\infty</script>注意中间最大化的式子就是确定$\alpha_i,\beta_j$的之后的结果，若$c_i(x) &gt; 0$，则令$\alpha_i \to +\infty$，如果$h_j(x) \not= 0$，很容易取值$\beta_j$使得$\beta_jh_j(x) \to +\infty$</li><li>考虑$x$，满足原来的约束，则$\theta_p(x) = \max \limits_{\alpha,\beta:\alpha_i \ge 0}[f(x)] = f(x)$，注意中间的最大化是确定$\alpha_i,\beta_j$的过程，$f(x)$就是个常量，常量的最大值就是本身<br>通过上面可以得出：<script type="math/tex; mode=display">\begin{eqnarray}f(x)=\begin{cases}f(x), &x满足原始问题的约束  \cr +\infty,&其他\end{cases}\end{eqnarray}</script>那么在满足约束条件下：<script type="math/tex; mode=display">\min \limits_{x} \theta_p(x) = \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \min \limits_{x}f(x)</script>即$\min \limits_{x} \theta_p(x)$与原始的优化问题等价，所以常用$\min \limits_{x} \theta_p(x)$代表原始问题，下表p表示原始问题，定义原始问题的最优值：<script type="math/tex; mode=display">\hat{p} = \min \limits_{x} \theta_p(x)</script></li></ul><h1 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h1><p>定义关于$\alpha,\beta$的函数：</p><script type="math/tex; mode=display">\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta)</script><p>等式的右边是关于$x$的函数的最小化，$x$确定后，最小值就只与$\alpha,\beta$有关了，所以是一个关于$\alpha,\beta$的函数。<br>考虑到极大化$\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta)$，即：</p><script type="math/tex; mode=display">\max \limits_{\alpha,\beta:\alpha_i \ge 0} \theta_D(\alpha,\beta) = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta)</script><p>这个就是原始问题的对偶问题，那么原始问题的表达式：</p><script type="math/tex; mode=display">\min \limits_{x} \theta_p(x) = \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta)</script><p>从形式上来看很对称，只不过原始问题是先固定的$L(x,\alpha,\beta)$中的$x$，优化出的参数$\alpha,\beta$，在优化最优$x$，而对偶问题是先固定$\alpha,\beta$，优化出最优的$x$，然后确定参数$\alpha,\beta$<br>定义对偶问题的最优值：</p><script type="math/tex; mode=display">\hat{d}= \max \limits_{\alpha,\beta:\alpha_i \ge 0}\theta_D(\alpha,\beta)</script><h1 id="原始问题和对偶问题的关系"><a href="#原始问题和对偶问题的关系" class="headerlink" title="原始问题和对偶问题的关系"></a>原始问题和对偶问题的关系</h1><blockquote><p><strong>定理</strong>：若原始问题与对偶问题都有最优值，则</p><script type="math/tex; mode=display">\begin{aligned} \hat{d} & = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta) \newline &\le \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \hat{p}\end{aligned}</script></blockquote><p>证明：对任意的$\alpha,\beta$和$x$，有：</p><script type="math/tex; mode=display">\theta_D(\alpha,\beta) = \min \limits_{x}L(x,\alpha,\beta) \le L(x,\alpha,\beta) \\ \le \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \min \limits_{x} \theta_p(x)</script><p>即：$\theta_D(\alpha,\beta) \le  \theta_p(x)$<br>由于原始问题与对偶问题都有最优值，所以：$\max \limits_{\alpha,\beta:\alpha_i \ge 0}\theta_D(\alpha,\beta) \le  \min \limits_{x}\theta_p(x)$<br>即：$\hat{d} = \max \limits_{\alpha,\beta:\alpha_i \ge 0} \min \limits_{x}L(x,\alpha,\beta) \le \min \limits_{x} \max \limits_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \hat{p} $<br>也就是说原始问题的最优值不小于对偶问题的最优值，但是我们要通过对偶问题来求解原始问题，就必须使得原始问题的最优值与对偶问题的最优值相等，于是可以得出下面的推论：</p><blockquote><p><strong> 推论</strong>：设$\hat{x}$和$\hat{\alpha}，\hat{\beta}$分别是原始问题和对偶问题的可行解，如果$\hat{d}$ ，那么$\hat{x}$和$\hat{\alpha},\hat{\beta}$分别是原始问题和对偶问题的最优解。</p></blockquote><p>所以，当原始问题和对偶问题的最优值相等：$\hat{d}=\hat{p}$ ，可以用求解对偶问题来求解原始问题（当然是对偶问题求解比直接求解原始问题简单的情况下），但是到底满足什么样的条件才能使得$\hat{d}=\hat{p}$，这就是KTT条件的来源</p><h1 id="KTT条件"><a href="#KTT条件" class="headerlink" title="KTT条件"></a>KTT条件</h1><blockquote><p><strong> 定理</strong>：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数(即由一阶多项式构成的函数，$f(x) =Ax+b$，A是矩阵，$x，b$是向量)；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有的$i$有$c_i(x) &lt; 0$，则存在$\hat{x}$和$\hat{\alpha}，\hat{\beta}$，使得$\hat{x}$ 是原始问题的最优解，$\hat{\alpha}，\hat{\beta}$是对问题的最优解，并且$\hat{d}=\hat{p}=L(\hat{x},\hat{\alpha},\hat{\beta})$</p><p><strong>定理</strong>：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数(即由一阶多项式构成的函数，$f(x) =Ax+b$，$A$是矩阵，$x，b$是向量)；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有的$i$有$c_i(x) &lt; 0$，则$\hat{x}$和$\hat{\alpha},\hat{\beta}$分别是原始问题和对偶问题的最优解的充分必要条件是$\hat{x}$和$\hat{\alpha}，\hat{\beta}$，满足下面的$Karush-Kuhn-Tucker(KKT)$     条件:</p><script type="math/tex; mode=display">\begin{aligned} &\nabla _x L(\hat{x},\hat{\alpha},\hat{\beta}) =0\newline   &\nabla _{\alpha}L(\hat{x},\hat{\alpha},\hat{\beta}) =0  \newline &\nabla _{\beta} L(\hat{x},\hat{\alpha},\hat{\beta}) =0\newline &\hat{\alpha_i}c_i(\hat{x})=0, i=1,2,...,k(KTT对偶互补条件)\newline &c_i(\hat{x}) \le 0,i=1,2...,k \newline &\hat{\alpha_i} \ge 0,i=1,2,...k \newline &h_j(\hat{x}) =0,j=1,2,...,l) \end{aligned}</script></blockquote><p>关于KKT 条件的理解：前面三个条件是由解析函数的知识，对于各个变量的偏导数为0（这就解释了一开始为什么假设三个函数连续可微，如果不连续可微的话，这里的偏导数存不存在就不能保证），后面四个条件就是原始问题的约束条件以及拉格朗日乘子需要满足的约束。</p><p>特别注意当$\hat{\alpha_i}\ge 0$时，由KKT对偶互补条件可知：$c_i(\hat{x}) =0$，这个知识点会在$ SVM$ 的推导中用到.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;在机器学习的算法理论推导中，在很多地方会用到拉格朗日乘数法，在支持向量机$SVM(Support Vector Machines)     
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="KTT" scheme="http://yoursite.com/tags/KTT/"/>
    
  </entry>
  
  <entry>
    <title>EM算法原理的理解</title>
    <link href="http://yoursite.com/2018/07/12/EM%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2018/07/12/EM算法/</id>
    <published>2018-07-12T12:47:12.953Z</published>
    <updated>2018-07-14T06:21:46.061Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。本文就对EM算法的原理做一个总结。<br>我们经常会从样本观察数据中，找出样本的模型参数。 最常用的方法就是极大化模型分布的对数似然函数。<br>　　　　但是在一些情况下，我们得到的观察数据有未观察到的隐含数据，此时我们未知的有隐含数据和模型参数，因而无法直接用极大化对数似然函数得到模型分布的参数。怎么办呢？这就是EM算法可以派上用场的地方了。<br>　　　　EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。<br>　　　　从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步，E步和M步。一轮轮迭代更新隐含数据和模型分布参数，直到收敛，即得到我们需要的模型参数。<br>　　　　一个最直观了解EM算法思路的是K-Means算法。在K-Means聚类时，每个聚类簇的质心是隐含数据。我们会假设K个初始化质心，即EM算法的E步；然后计算得到每个样本最近的质心，并把样本聚类到最近的这个质心，即EM算法的M步。重复这个E步和M步，直到质心不再变化为止，这样就完成了K-Means聚类。<br>　　　　当然，K-Means算法是比较简单的，实际中的问题往往没有这么简单。上面对EM算法的描述还很粗糙，我们需要用数学的语言精准描述。</p><h1 id="EM算法的推导"><a href="#EM算法的推导" class="headerlink" title="EM算法的推导"></a>EM算法的推导</h1><p>对于$m$个样本观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，找出样本的模型参数$θ$, 极大化模型分布的对数似然函数如下：</p><script type="math/tex; mode=display">\begin{align}L(\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m logP(x^{(i)}|\theta)\end{align}</script><p>如果我们得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},…z^{(m)})$，此时我们的极大化模型分布的对数似然函数如下：</p><script type="math/tex; mode=display">\begin{align}L(\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m logP(x^{(i)}|\theta) = arg \max \limits_{\theta}\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}， z^{(i)}|\theta)\end{align}</script><p>上面这个式子是没有 办法直接求出$θ$的。因此需要一些特殊的技巧， 我们首先对这个式子进行缩放如下：</p><script type="math/tex; mode=display">\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}， z^{(i)}|\theta)   & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} \\  \end{align} .......(1)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & & & & & &\geq  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} \end{align}........(2)</script><p>　上面第(1)式引入了一个未知的新的分布$Q_i(z^{(i)})$，第(2)式用到了$Jensen$不等式：<script type="math/tex">\begin{align} log\sum\limits_j\lambda_jy_j \geq \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geq 0, \sum\limits_j\lambda_j =1\end{align}</script><br>或者说由于对数函数是凹函数，所以有:</p><script type="math/tex; mode=display">f(E(x)) \geq E(f(x))\;\;如果f(x) 是凹函数</script><p>此时如果要满足$Jensen$不等式的等号，则有：</p><script type="math/tex; mode=display">\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})} =c, c为常数</script><p>　由于$Q_i(z^{(i)})$是一个分布，所以满足：</p><script type="math/tex; mode=display">\sum\limits_{z}Q_i(z^{(i)}) =1</script><p>　从上面两式，我们可以得到：</p><script type="math/tex; mode=display">Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)}|\theta)}{\sum\limits_{z}P(x^{(i)}， z^{(i)}|\theta)} =  \frac{P(x^{(i)}， z^{(i)}|\theta)}{P(x^{(i)}|\theta)} = P( z^{(i)}|x^{(i)}，\theta))</script><p>如果$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}，\theta))$, 则第(2)式是我们的包含隐藏数据的对数似然的一个下界。<br>如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：</p><script type="math/tex; mode=display">arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)}|\theta)}{Q_i(z^{(i)})}</script><p>去掉上式中为常数的部分，则我们需要极大化的对数似然下界为：</p><script type="math/tex; mode=display">\begin{align} arg \max \limits_{\theta} \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}\end{align}</script><p>上式也就是我们的EM算法的M步，那E步呢？注意到上式中$Q_i(z^{(i)})$是一个分布，因此$\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}$可以理解为$logP(x^{(i)}， z^{(i)}|\theta)$基于条件概率分布$Q_i(z^{(i)})$的期望。<br>至此，我们理解了EM算法中E步和M步的具体数学含义。</p><h1 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h1><p>EM算法给出了一种很有效的最大似然估计的方法：<strong>重复地构造$L(\theta)$的下界（E步），然后最大化这个下界（M步）</strong><br>现在我们总结下EM算法的流程。<br>输入：观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，联合分布$p(x,z |\theta)$条件分布$p(z|x, \theta)$最大迭代次数$J$。<br>1) 随机初始化模型参数θ的初值$\theta^{0}$<br>2） for j  from 1 to J开始EM算法迭代：</p><blockquote><p>  a) E步：计算联合分布的条件概率期望：</p><script type="math/tex; mode=display">Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}，\theta^{j}))</script><script type="math/tex; mode=display">\begin{align} L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}， z^{(i)}|\theta)}\end{align}</script><p>  b) M步：极大化$L(\theta, \theta^{j})$,得到$\theta^{j+1}$</p><script type="math/tex; mode=display">\begin{align} \theta^{j+1} = arg \max \limits_{\theta}L(\theta, \theta^{j})\end{align}</script><p>  c) 如果$θ^{j+1}$已收敛，则算法结束。否则继续回到步骤a)进行E步迭代。</p></blockquote><p>输出：模型参数$θ$。</p><h1 id="EM算法的收敛性思考"><a href="#EM算法的收敛性思考" class="headerlink" title="EM算法的收敛性思考"></a>EM算法的收敛性思考</h1><p>EM算法的流程并不复杂，但是还有两个问题需要我们思考：<br>　　　　1） EM算法能保证收敛吗？<br>　　　　2） EM算法如果收敛，那么能保证收敛到全局最大值吗？　　<br>　　　　首先我们来看第一个问题, EM算法的收敛性。要证明EM算法收敛，则我们需要证明我们的对数似然函数的值在迭代的过程中一直在增大(<code>说白了就证明单调性</code>)。即：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1}) \geq \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j})</script><p>由于</p><script type="math/tex; mode=display">L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j}))log{P(x^{(i)}， z^{(i)}|\theta)}</script><p>　令：<script type="math/tex">H(\theta, \theta^{j}) =  \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j}))log{P( z^{(i)}|x^{(i)}，\theta)}</script><br>上两式相减得到：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta) = L(\theta, \theta^{j}) - H(\theta, \theta^{j})</script><p>在上式中分别取$θ$为$θ^j$和$θ^{j+1}$并相减得到：</p><script type="math/tex; mode=display">\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1})  - \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j}) = [L(\theta^{j+1}, \theta^{j}) - L(\theta^{j}, \theta^{j}) ] -[H(\theta^{j+1}, \theta^{j}) - H(\theta^{j}, \theta^{j}) ]</script><p>要证明EM算法的收敛性，我们只需要证明上式的右边是非负的即可。<br>由于$θ^{j+1}$使得$L(\theta, \theta^{j})$极大，因此有:</p><script type="math/tex; mode=display">L(\theta^{j+1}, \theta^{j}) - L(\theta^{j}, \theta^{j})  \geq 0</script><p>而对于第二部分，我们有：</p><script type="math/tex; mode=display">\begin{align} H(\theta^{j+1}, \theta^{j}) - H(\theta^{j}, \theta^{j})  & = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j})log\frac{P( z^{(i)}|x^{(i)}，\theta^{j+1})}{P( z^{(i)}|x^{(i)}，\theta^j)} \\ \end{align}...... (4)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & & & & & & \leq  \sum\limits_{i=1}^mlog(\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j})\frac{P( z^{(i)}|x^{(i)}，\theta^{j+1})}{P( z^{(i)}|x^{(i)}，\theta^j)}) \\ \end{align} ...... (5)</script><script type="math/tex; mode=display">\begin{align}  & & & & & & & &  = \sum\limits_{i=1}^mlog(\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}，\theta^{j+1})) = 0  \end{align}...... (6)</script><p>其中第（4）式用到了$Jensen$不等式，只不过和第二节的使用相反而已，第（5）式用到了概率分布累积为1的性质。<br>　　　　至此，我们得到了：$\sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j+1})  - \sum\limits_{i=1}^m logP(x^{(i)}|\theta^{j})  \geq 0$, 证明了EM算法的收敛性。<br>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法这样的迭代算法相同。至此我们也回答了上面提到的第二个问题。</p><h1 id="EM的一点思考"><a href="#EM的一点思考" class="headerlink" title="EM的一点思考"></a>EM的一点思考</h1><p>所谓EM算法就是在含有隐变量的时候，把隐变量的分布设定为一个以观测变量为前提条件的后验分布，使得参数的似然函数与其下界相等，通过极大化这个下界来极大化似然函数，从避免直接极大化似然函数过程中因为隐变量未知而带来的困难！<code>EM算法主要是两步,E步选择出合适的隐变量分布（一个以观测变量为前提条件的后验分布），使得参数的似然函数与其下界相等；M步：极大化似然函数的下界，拟合出参数</code>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="EM" scheme="http://yoursite.com/tags/EM/"/>
    
  </entry>
  
  <entry>
    <title>关于Adaboost的原理的理解</title>
    <link href="http://yoursite.com/2018/07/09/Adaboosting/"/>
    <id>http://yoursite.com/2018/07/09/Adaboosting/</id>
    <published>2018-07-09T13:50:01.378Z</published>
    <updated>2018-07-14T05:54:06.264Z</updated>
    
    <content type="html"><![CDATA[<h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;假设给出样本的形式为 \( X =(x_1,x_2,\cdots,x_d) \),  每个样本对应一个决策值 \( Y=y \),那么可以定义一个实例\( (X,Y) = (x_1,x_2,\cdots,x_d,y)\)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;算法流程&quot;&gt;&lt;a href=&quot;#算法流程&quot; class=&quot;headerlink&quot; title=&quot;算法流程&quot;&gt;&lt;/a&gt;算法流程&lt;/h1&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;假设给出样本的形式为 \( X =(x_1,x_2,\cdots,x_d) 
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://yoursite.com/categories/Machine-Learning/"/>
    
    
      <category term="Adaboost" scheme="http://yoursite.com/tags/Adaboost/"/>
    
  </entry>
  
  <entry>
    <title>关于java的NIO的理解</title>
    <link href="http://yoursite.com/2018/07/07/%E5%85%B3%E4%BA%8EJavaNIO%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2018/07/07/关于JavaNIO的一些理解/</id>
    <published>2018-07-07T08:49:37.402Z</published>
    <updated>2018-07-14T06:21:27.797Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-传统的I-O"><a href="#1-传统的I-O" class="headerlink" title="1.传统的I/O"></a>1.传统的I/O</h1><p>&nbsp;&nbsp;使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socket), 如下程序:<br>File.read(fileDesc, buf, len);<br>Socket.send(socket, buf, len);<br>会有较大的性能开销, 主要表现在一下两方面:  </p><ol><li>上下文切换(context switch), 此处有4次用户态和内核态的切换  </li><li>Buffer内存开销, 一个是应用程序buffer, 另一个是系统读取buffer以及socket buffer<br>其运行示意图如下:  </li></ol><p><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/02/NIO/001.png" alt="git_image">  </p><p>&nbsp;&nbsp;1) 先将文件内容从磁盘中拷贝到操作系统buffer.<br>&nbsp;&nbsp;2) 再从操作系统buffer拷贝到程序应用buffer.<br>&nbsp;&nbsp;3) 从程序buffer拷贝到socket buffer.<br>&nbsp;&nbsp;4) 从socket buffer拷贝到协议引擎.  </p><h1 id="2-NIO"><a href="#2-NIO" class="headerlink" title="2. NIO"></a>2. NIO</h1><p>&nbsp;&nbsp;NIO技术省去了将操作系统的read buffer拷贝到程序的buffer, 以及从程序buffer拷贝到socket buffer的步骤, 直接将 read buffer 拷贝到 socket buffer. java 的 FileChannel.transferTo() 方法就是这样的实现, 这个实现是依赖于操作系统底层的sendFile()实现的.<br>publicvoid transferTo(long position, long count, WritableByteChannel target);<br>他的底层调用的是系统调用sendFile()方法<br>sendfile(int out_fd, int in_fd, off_t *offset, size_t count);<br>如下图:<br><img src="https://raw.githubusercontent.com/shunj-g/shunj-g.github.io/master/images/my_images/02/NIO/002.png" alt="git_image"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-传统的I-O&quot;&gt;&lt;a href=&quot;#1-传统的I-O&quot; class=&quot;headerlink&quot; title=&quot;1.传统的I/O&quot;&gt;&lt;/a&gt;1.传统的I/O&lt;/h1&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socke
      
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="NIO" scheme="http://yoursite.com/tags/NIO/"/>
    
  </entry>
  
  <entry>
    <title>如何使用github搭建个人博客</title>
    <link href="http://yoursite.com/2018/07/03/github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2018/07/03/github搭建个人博客/</id>
    <published>2018-07-03T14:19:20.057Z</published>
    <updated>2018-07-14T06:21:38.780Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装git和注册github账号"><a href="#安装git和注册github账号" class="headerlink" title="安装git和注册github账号"></a>安装git和注册github账号</h2><h3 id="第一步：安装下载git"><a href="#第一步：安装下载git" class="headerlink" title="第一步：安装下载git"></a>第一步：安装下载git</h3><ul><li><p>下载<a href="https://git-for-windows.github.io/" target="_blank" rel="noopener">git</a></p></li><li><p>安装git及步骤</p></li><li>全选<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-4da9b2db5ad099b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/519" alt="git_image"></div></li><li>选择圈住的部分<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-44bddccbb0bc44fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/483" alt="git_image"></div></li></ul><p><code>余下的一路默认</code></p><h3 id="第二步：安装node-js"><a href="#第二步：安装node-js" class="headerlink" title="第二步：安装node.js"></a>第二步：安装node.js</h3><ul><li><p>node.js下载 <a href="https://nodejs.org/en/" target="_blank" rel="noopener">下载地址</a></p></li><li><p>安装步骤：一路默认即可</p></li></ul><h3 id="第三步：安装Hexo"><a href="#第三步：安装Hexo" class="headerlink" title="第三步：安装Hexo"></a>第三步：安装Hexo</h3><ul><li><p>利用 npm 命令即可安装。在任意位置点击鼠标右键，选择Git Bash<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-ed306496f5c34312.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/237" alt="git_image"></div></p></li><li><p>输入命令：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo</span><br></pre></td></tr></table></figure><p><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-0b9d56643f6b27cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/648" alt="git_image"></div></p><p><code>注意：-g是指全局安装hexo。</code></p><h3 id="第四步：初始化Hexo"><a href="#第四步：初始化Hexo" class="headerlink" title="第四步：初始化Hexo"></a>第四步：初始化Hexo</h3><ul><li>创建文件夹（我的是在E盘创建的Hexo）<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-b4cfc9da3f2063b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/650" alt=""></div></li></ul><p><code>根据个人爱好创建博客文件夹</code> </p><ul><li>在Hexo文件下，右键运行Git Bash，输入命令：hexo init<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-f6ae9b7089741c89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></div></li></ul><p><code>这里可能时间会长些，要耐心等待</code></p><p><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-d0452912537c03e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/658" alt=""></div></p><p><code>初始化成功后生成的一些列文件</code></p><ul><li>在_config.yml,进行基础配置<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-cd5743eda172deca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/663" alt=""></div></li></ul><p>其中可以在这里浏览更多<a href="https://hexo.io/themes/" target="_blank" rel="noopener">主题</a>，然后在Hexo文件夹下 Git Bash</p><p>输入命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span>  https://github.com/iissnan/hexo-theme-next/</span><br></pre></td></tr></table></figure><p>[next为主题名字]，来获得更多主题</p><ul><li>本地浏览博客</li></ul><p>分别输入 如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>这里有更多<a href="https://segmentfault.com/a/1190000002632530" target="_blank" rel="noopener">hexo常用命令</a></p><ul><li>写文章<br>在E:\Hexo\source_posts文件下，新建.md文件就可以写文章<br><div align="center"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-f369abde30af73e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/478" alt=""></div></li></ul><h2 id="部署到Github上"><a href="#部署到Github上" class="headerlink" title="部署到Github上"></a>部署到Github上</h2><ul><li><p>申请Github账号，（注意别忘了进行账号邮箱验证） <code>一般使用的是Google邮箱</code></p></li><li><p>new repository</p></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/1531909-8decffce7d3866b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/494" alt="git_image"></p><ul><li><p>_config.yml进行配置 </p></li><li><p>发布到Github</p></li></ul><p>输入如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/1531909-72b3c30ffbfb1210.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/274" alt="s"><br><img src="https://upload-images.jianshu.io/upload_images/1531909-1f99441c5f2e0cfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/359" alt="s"></p><ul><li>测试访问<br>在浏览器输入：<a href="https://xxxxx.github.io/" target="_blank" rel="noopener">https://xxxxx.github.io/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;安装git和注册github账号&quot;&gt;&lt;a href=&quot;#安装git和注册github账号&quot; class=&quot;headerlink&quot; title=&quot;安装git和注册github账号&quot;&gt;&lt;/a&gt;安装git和注册github账号&lt;/h2&gt;&lt;h3 id=&quot;第一步：安装下载g
      
    
    </summary>
    
      <category term="Github" scheme="http://yoursite.com/categories/Github/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="node.js" scheme="http://yoursite.com/tags/node-js/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/07/03/hello-world/"/>
    <id>http://yoursite.com/2018/07/03/hello-world/</id>
    <published>2018-07-03T00:27:50.191Z</published>
    <updated>2018-07-14T06:21:30.275Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
